"""
æ‰¹é‡æ“ä½œå·¥å…·æ¨¡å— - SQLModelä¼˜åŒ–ç‰ˆæœ¬
æä¾›é«˜æ•ˆçš„æ‰¹é‡æ’å…¥ã€æ›´æ–°å’Œåˆ é™¤åŠŸèƒ½ï¼Œæ”¯æŒMySQLçš„UPSERTæ“ä½œ
è‡ªåŠ¨æ¨æ–­å”¯ä¸€é”®çº¦æŸï¼Œæ™ºèƒ½å¤„ç†æ•°æ®å†²çªï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§
"""

import math
import os
import time
import random
from collections import defaultdict
from dataclasses import dataclass
from datetime import datetime, date
from functools import lru_cache, wraps
from typing import List, Dict, Any, Type, Set, Optional, Tuple, Callable

from loguru import logger
from sqlalchemy import func, UniqueConstraint, Index
from sqlalchemy.dialects.mysql import insert as mysql_insert
from sqlalchemy.exc import OperationalError
from sqlalchemy.sql import literal_column
from sqlmodel import Session

# é¡¹ç›®å†…éƒ¨å¯¼å…¥
from app.models import db_session_context


@dataclass
class BatchOpsConfig:
    """æ‰¹é‡å†™åº“å¯è°ƒå‚æ•°é…ç½®ï¼ˆå•è¿›ç¨‹å†…å¸¸é‡ï¼Œå¯æŒ‰éœ€æ‰©å±•ä¸ºå¯çƒ­æ›´ï¼‰ã€‚"""
    base_batch: int = 200  # é»˜è®¤æ‰¹é‡å¤§å°ï¼ˆé™ä½ä»¥å‡å°‘é”ç«äº‰ï¼Œå¯è¢«è°ƒç”¨è€… batch_size è¦†ç›–ï¼‰


# ğŸš€ ä¼˜åŒ–ï¼šç®€åŒ–é…ç½®åŠ è½½é€»è¾‘
def _load_batch_ops_config() -> BatchOpsConfig:
    """åŠ è½½æ‰¹é‡æ“ä½œé…ç½® - ä¼˜å…ˆçº§ï¼šSyncStrategyConfig > ç¯å¢ƒå˜é‡ > é»˜è®¤å€¼"""
    default_config = BatchOpsConfig()
    
    # 1. å°è¯•ä» SyncStrategyConfig è¯»å–
    try:
        from ...services.management.sync_strategy_config import SyncStrategyConfig  # type: ignore
        config_base_batch = getattr(SyncStrategyConfig, "BATCH_OPS_BASE_BATCH", None)
        if config_base_batch is not None:
            default_config.base_batch = int(config_base_batch)
    except (ImportError, AttributeError, ValueError) as e:
        logger.debug(f"æ— æ³•ä»SyncStrategyConfigåŠ è½½é…ç½®: {e}")
    
    # 2. ç¯å¢ƒå˜é‡è¦†ç›–
    try:
        env_base_batch = os.getenv("BATCH_OPS_BASE_BATCH")
        if env_base_batch is not None:
            default_config.base_batch = int(env_base_batch)
    except ValueError as e:
        logger.warning(f"ç¯å¢ƒå˜é‡BATCH_OPS_BASE_BATCHå€¼æ— æ•ˆ: {e}")
    
    return default_config


_BATCH_OPS_CONFIG = _load_batch_ops_config()


class BatchOperations:
    """æ‰¹é‡æ“ä½œå·¥å…·ç±» - SQLModelä¼˜åŒ–ç‰ˆæœ¬"""
    
    # ğŸš€ é…ç½®ç»Ÿä¸€ï¼šä½¿ç”¨DAOConfigä¸­çš„é…ç½®ï¼Œé¿å…é‡å¤å®šä¹‰
    from ..dao_config import DAOConfig
    MAX_BATCH_SIZE = 2000  # æœ€å¤§æ‰¹æ¬¡å¤§å°ï¼Œé˜²æ­¢æ­»é”
    DEFAULT_BATCH_SIZE = DAOConfig.DEFAULT_BATCH_SIZE  # ä½¿ç”¨ç»Ÿä¸€é…ç½®
    
    # ğŸš€ å¸¸é‡å®šä¹‰ï¼šä¸å¯æ›´æ–°çš„å­—æ®µé›†åˆ
    NON_UPDATEABLE_FIELDS = frozenset({
        "id", "created_at", "updated_at"
    })  # ç³»ç»Ÿå­—æ®µï¼Œä¸åº”åœ¨UPSERTæ—¶æ›´æ–°
    
    # ğŸš€ å¸¸é‡å®šä¹‰ï¼šç¼“å­˜é…ç½®
    CACHE_SIZE_SMALL = 256  # å°å‹ç¼“å­˜å¤§å°
    CACHE_SIZE_LARGE = 512  # å¤§å‹ç¼“å­˜å¤§å°
    DEFAULT_UPSERT_BATCH_SIZE = 200  # é»˜è®¤UPSERTæ‰¹æ¬¡å¤§å°ï¼ˆé™ä½ä»¥å‡å°‘é”ç«äº‰ï¼‰
    
    # ğŸš€ æ­»é”é‡è¯•é…ç½®
    MAX_DEADLOCK_RETRIES = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°
    DEADLOCK_RETRY_DELAY_BASE = 0.1  # åŸºç¡€å»¶è¿Ÿï¼ˆç§’ï¼‰
    DEADLOCK_RETRY_DELAY_MAX = 2.0  # æœ€å¤§å»¶è¿Ÿï¼ˆç§’ï¼‰
    
    @staticmethod
    def _safe_sort_batch_rows(batch_rows: List[Dict[str, Any]], unique_keys: List[str]) -> List[Dict[str, Any]]:
        """å®‰å…¨åœ°å¯¹æ‰¹æ¬¡æ•°æ®è¿›è¡Œæ’åºï¼Œé™ä½æ­»é”æ¦‚ç‡
        
        Args:
            batch_rows: æ‰¹æ¬¡æ•°æ®
            unique_keys: å”¯ä¸€é”®åˆ—è¡¨
            
        Returns:
            æ’åºåçš„æ‰¹æ¬¡æ•°æ®
        """
        try:
            def _key_fn(r: Dict[str, Any]):
                return tuple(str(r.get(k)) for k in unique_keys)
            return sorted(batch_rows, key=_key_fn)
        except Exception as e:
            logger.warning(f"æ‰¹æ¬¡æ•°æ®æ’åºå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹é¡ºåº: {e}")
            return batch_rows
    
    @staticmethod
    def _execute_upsert_and_calculate_stats(db: Session, insert_stmt, update_cols: Dict, batch_size: int) -> Tuple[int, int]:
        """æ‰§è¡ŒUPSERTæ“ä½œå¹¶è®¡ç®—ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¸¦æ­»é”è‡ªåŠ¨é‡è¯•ï¼‰
        
        å½“å‘ç”Ÿæ­»é”(Error 1213)æˆ–é”ç­‰å¾…è¶…æ—¶(Error 1205)æ—¶ï¼Œè‡ªåŠ¨é‡è¯•
        ä½¿ç”¨æŒ‡æ•°é€€é¿ç­–ç•¥ï¼Œå¹¶æ·»åŠ éšæœºæŠ–åŠ¨é¿å…å¤šä¸ªäº‹åŠ¡åŒæ—¶é‡è¯•
        
        Args:
            db: æ•°æ®åº“ä¼šè¯
            insert_stmt: æ’å…¥è¯­å¥
            update_cols: æ›´æ–°åˆ—å­—å…¸
            batch_size: æ‰¹æ¬¡å¤§å°
            
        Returns:
            (æ’å…¥æ•°é‡, æ›´æ–°æ•°é‡)çš„å…ƒç»„
        """
        last_exception = None
        
        for attempt in range(BatchOperations.MAX_DEADLOCK_RETRIES):
            try:
                res = db.execute(insert_stmt.on_duplicate_key_update(**update_cols))
                affected = int(getattr(res, "rowcount", 0) or 0)
                approx_updated = max(0, affected - batch_size)
                approx_inserted = max(0, batch_size - approx_updated)
                return approx_inserted, approx_updated
                
            except OperationalError as e:
                error_code = getattr(e.orig, 'args', [None])[0]
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ­»é”(1213)æˆ–é”ç­‰å¾…è¶…æ—¶(1205)
                if error_code in (1213, 1205):
                    last_exception = e
                    
                    if attempt < BatchOperations.MAX_DEADLOCK_RETRIES - 1:
                        # è®¡ç®—é€€é¿å»¶è¿Ÿï¼šæŒ‡æ•°å¢é•¿ + éšæœºæŠ–åŠ¨
                        delay = min(
                            BatchOperations.DEADLOCK_RETRY_DELAY_BASE * (2 ** attempt),
                            BatchOperations.DEADLOCK_RETRY_DELAY_MAX
                        )
                        jitter = random.uniform(0, delay * 0.3)  # 30%éšæœºæŠ–åŠ¨
                        total_delay = delay + jitter
                        
                        logger.warning(
                            f"æ­»é”æ£€æµ‹åˆ°(é”™è¯¯{error_code})ï¼Œç¬¬{attempt + 1}/{BatchOperations.MAX_DEADLOCK_RETRIES}æ¬¡é‡è¯•ï¼Œ"
                            f"ç­‰å¾…{total_delay:.2f}ç§’åé‡è¯•"
                        )
                        time.sleep(total_delay)
                        continue
                    else:
                        logger.error(f"æ­»é”é‡è¯•{BatchOperations.MAX_DEADLOCK_RETRIES}æ¬¡åä»å¤±è´¥: {e}")
                        raise
                else:
                    # éæ­»é”é”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
                    logger.error(f"ç”Ÿæˆå¼ upsert æ‰§è¡Œå¤±è´¥: {e}")
                    raise
                    
            except Exception as e:
                logger.error(f"ç”Ÿæˆå¼ upsert æ‰§è¡Œå¤±è´¥: {e}")
                raise
        
        # ç†è®ºä¸Šä¸ä¼šåˆ°è¿™é‡Œï¼Œä½†ä¸ºäº†å®‰å…¨
        if last_exception:
            raise last_exception
    
    @staticmethod
    def _get_present_columns(batch_rows: List[Dict[str, Any]]) -> Set[str]:
        """è·å–æ‰¹æ¬¡æ•°æ®ä¸­å®é™…å­˜åœ¨çš„åˆ—å
        
        Args:
            batch_rows: æ‰¹æ¬¡æ•°æ®
            
        Returns:
            å­˜åœ¨çš„åˆ—åé›†åˆ
        """
        present_cols = set()
        try:
            for r in batch_rows:
                present_cols.update(r.keys())
        except Exception as e:
            logger.warning(f"è·å–åˆ—åå¤±è´¥: {e}")
        return present_cols
    
    @staticmethod
    def _build_update_expression(table, table_name: str, column_name: str) -> str:
        """æ„å»ºæ¡ä»¶æ›´æ–°è¡¨è¾¾å¼
        
        Args:
            table: è¡¨å¯¹è±¡
            table_name: è¡¨å
            column_name: åˆ—å
            
        Returns:
            æ›´æ–°è¡¨è¾¾å¼å­—ç¬¦ä¸²
        """
        try:
            col = table.columns[column_name]
            col_name = col.name if not hasattr(col, 'key') else col.key
            # ä½¿ç”¨ IFNULL é¿å… NULL å€¼è¦†ç›–åŸå€¼
            return f"IFNULL(VALUES(`{col_name}`), `{table_name}`.`{col_name}`)"
        except Exception as e:
            logger.error(f"æ„é€ æ›´æ–°è¡¨è¾¾å¼å¤±è´¥ for column {column_name}: {e}")
            return None
    
    @staticmethod
    def _parse_date_string(date_str: str) -> date:
        """è§£ææ—¥æœŸå­—ç¬¦ä¸²ä¸ºdateå¯¹è±¡
        
        Args:
            date_str: YYYYMMDDæ ¼å¼çš„æ—¥æœŸå­—ç¬¦ä¸²
            
        Returns:
            dateå¯¹è±¡
        """
        return datetime.strptime(date_str, "%Y%m%d").date()
    

    @staticmethod
    def bulk_upsert_mysql_generated(
            table_model: Type,
            data: List[Dict[str, Any]],
            batch_size: int = DEFAULT_UPSERT_BATCH_SIZE,
            enable_updated_at: bool = True,
    ) -> Dict[str, int]:
        """ä½¿ç”¨ MySQL ç”Ÿæˆå¼ upsertï¼ˆINSERT ... ON DUPLICATE KEY UPDATEï¼‰è¿›è¡Œæ‰¹é‡å†™å…¥ã€‚

        - è‡ªåŠ¨æ¨æ–­å”¯ä¸€é”®ï¼›
        - é’ˆå¯¹æ¯æ‰¹æ•°æ®æ„é€ ä¸€æ¬¡ insert + on duplicate è¯­å¥ï¼›
        - ç®€åŒ–ç‰ˆï¼šä¸åšå˜æ›´æ„ŸçŸ¥ã€ä¸åšå˜æ›´é¡¹è·Ÿè¸ªã€‚
        - ğŸš€ ä¼˜åŒ–ï¼šç»Ÿä¸€ä½¿ç”¨SQLModelä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œç®€åŒ–API

        Args:
            table_model: è¡¨æ¨¡å‹ç±»
            data: æ•°æ®åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            enable_updated_at: æ˜¯å¦è‡ªåŠ¨æ›´æ–° updated_at å­—æ®µ

        Returns: {"inserted": int, "updated": int, "total": int}
        """
        if not data:
            return {"inserted": 0, "updated": 0, "total": 0}

        # ğŸš€ SQLModelä¼˜åŒ–ï¼šç»Ÿä¸€ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œç®€åŒ–APIè®¾è®¡
        with db_session_context() as db:
            return BatchOperations._execute_bulk_upsert(
                db, table_model, data, batch_size, enable_updated_at
            )
    
    @staticmethod
    def _execute_bulk_upsert(
            db: Session, table_model: Type, data: List[Dict[str, Any]], 
            batch_size: int, enable_updated_at: bool
    ) -> Dict[str, int]:
        """æ‰§è¡Œæ‰¹é‡upsertçš„æ ¸å¿ƒé€»è¾‘ - å†…éƒ¨æ–¹æ³•
        
        Args:
            db: æ•°æ®åº“ä¼šè¯å¯¹è±¡
            table_model: è¡¨æ¨¡å‹ç±»
            data: æ•°æ®åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            enable_updated_at: æ˜¯å¦è‡ªåŠ¨æ›´æ–°updated_atå­—æ®µ
            
        Returns:
            åŒ…å«æ’å…¥ã€æ›´æ–°ç»Ÿè®¡çš„å­—å…¸
        """
        try:
            # ğŸš€ ä¼˜åŒ–ï¼šç›´æ¥ä½¿ç”¨ç¼“å­˜æ–¹æ³•ï¼Œé¿å…ä¸å¿…è¦çš„ç±»å‹è½¬æ¢
            unique_keys_tuple = BatchOperations._infer_unique_keys_from_model_cached(table_model)
            if not unique_keys_tuple:
                raise ValueError(f"æ¨¡å‹ {table_model.__name__} ç¼ºå°‘ä¸šåŠ¡å”¯ä¸€é”®çº¦æŸ")
            unique_keys = list(unique_keys_tuple)

            table = table_model.__table__
            unique_set = set(unique_keys)

            total_inserted = 0
            total_updated = 0
            total_count = 0

            # ğŸš€ ä¼˜åŒ–ï¼šä½¿ç”¨ç±»å¸¸é‡ï¼Œé™ä½æ­»é”æ¦‚ç‡
            eff_batch = max(1, min(int(batch_size or BatchOperations.DEFAULT_BATCH_SIZE), BatchOperations.MAX_BATCH_SIZE))

            # ğŸš€ ä¼˜åŒ–ï¼šè·å–è¡¨çš„æœ‰æ•ˆåˆ—åé›†åˆï¼Œç”¨äºè¿‡æ»¤æ•°æ®ä¸­ä¸å­˜åœ¨çš„åˆ—
            valid_columns = {col.name for col in table.columns}

            for i in range(0, len(data), eff_batch):
                batch_rows = data[i: i + eff_batch]
                if not batch_rows:
                    continue
                total_count += len(batch_rows)

                # ğŸš€ ä¼˜åŒ–ï¼šè¿‡æ»¤æ‰è¡¨ä¸­ä¸å­˜åœ¨çš„åˆ—ï¼Œé¿å… "Unconsumed column names" é”™è¯¯
                batch_rows = [
                    {k: v for k, v in row.items() if k in valid_columns}
                    for row in batch_rows
                ]

                # ğŸš€ ä¼˜åŒ–ï¼šä½¿ç”¨å®‰å…¨æ’åºæ–¹æ³•ï¼Œç¡®ä¿é”è·å–é¡ºåºä¸€è‡´ï¼Œé™ä½æ­»é”æ¦‚ç‡
                batch_rows = BatchOperations._safe_sort_batch_rows(batch_rows, unique_keys)

                # ç”Ÿæˆ insert è¯­å¥
                insert_stmt = mysql_insert(table).values(batch_rows)

                
                update_cols = {}
                upd_names = BatchOperations._get_update_column_names_cached(table_model, tuple(sorted(unique_set)))
                # è·å–è¡¨åï¼Œç”¨äºåœ¨ UPDATE å­å¥ä¸­é™å®šåˆ—ï¼Œé¿å…æ­§ä¹‰
                table_name = table.name
                
                # æ–¹æ¡ˆ3ï¼šæ¡ä»¶æ›´æ–° SQL
                # å‰æï¼šä¸šåŠ¡å±‚å·²ç»ç»Ÿä¸€å­—æ®µç»“æ„ï¼Œæ‰€æœ‰è®°å½•éƒ½åŒ…å«ç›¸åŒçš„å­—æ®µé›†åˆ
                # - æ–°è®°å½•ï¼šåŸºç¡€å­—æ®µæœ‰å€¼ï¼Œç«ä»·å­—æ®µæœ‰å€¼
                # - æ—§è®°å½•ï¼šåŸºç¡€å­—æ®µä¸º NULLï¼Œç«ä»·å­—æ®µæœ‰å€¼
                # 
                # æ›´æ–°ç­–ç•¥ï¼šæ‰€æœ‰å­—æ®µéƒ½ä½¿ç”¨æ¡ä»¶æ›´æ–°
                # IFNULL(VALUES(col), table.col) è¡¨ç¤ºï¼š
                # - å¦‚æœæ–°å€¼ä¸ä¸º NULLï¼Œä½¿ç”¨æ–°å€¼
                # - å¦‚æœæ–°å€¼ä¸º NULLï¼Œä¿æŒåŸå€¼ï¼ˆä¸æ›´æ–°ï¼‰
                # è¿™æ ·å¯ä»¥é¿å… NULL å€¼è¦†ç›–æ•°æ®åº“ä¸­çš„åŸå€¼
                
                # ğŸš€ ä¼˜åŒ–ï¼šä½¿ç”¨è¾…åŠ©æ–¹æ³•è·å–å­˜åœ¨çš„åˆ—
                present_cols = BatchOperations._get_present_columns(batch_rows)
                effective_upd_names = [c for c in upd_names if c in present_cols]
                
                # ğŸš€ ä¼˜åŒ–ï¼šä½¿ç”¨è¾…åŠ©æ–¹æ³•æ„å»ºæ›´æ–°è¡¨è¾¾å¼
                for c in effective_upd_names:
                    expr = BatchOperations._build_update_expression(table, table_name, c)
                    if expr:
                        update_cols[c] = literal_column(expr)
                if enable_updated_at and ("updated_at" in BatchOperations._get_model_columns_cached(table_model)):
                    update_cols["updated_at"] = func.now()

                if not update_cols:
                    # ğŸš€ ä¼˜åŒ–ï¼šä½¿ç”¨è¾…åŠ©æ–¹æ³•æ‰§è¡ŒINSERT IGNORE
                    inserted = BatchOperations._execute_insert_ignore(db, insert_stmt, len(batch_rows))
                    total_inserted += inserted
                    continue

                # ğŸš€ ä¼˜åŒ–ï¼šä½¿ç”¨è¾…åŠ©æ–¹æ³•æ‰§è¡ŒUPSERTå¹¶è®¡ç®—ç»Ÿè®¡
                batch_inserted, batch_updated = BatchOperations._execute_upsert_and_calculate_stats(
                    db, insert_stmt, update_cols, len(batch_rows)
                )
                total_inserted += batch_inserted
                total_updated += batch_updated

            # SQLModelä¼šè¯ä¼šè‡ªåŠ¨æäº¤/å›æ»šï¼Œä¸éœ€è¦æ‰‹åŠ¨ç®¡ç†
            return {
                "inserted": int(total_inserted),
                "updated": int(total_updated),
                "total": int(total_count)
            }
        except Exception as e:
            logger.error(f"æ‰¹é‡upsertå¤±è´¥: {e}")
            raise

    @staticmethod
    def _sanitize_value(value: Any) -> Any:
        """å°†ä¸å¯å†™å…¥çš„å€¼ï¼ˆå¦‚ NaNï¼‰è½¬æ¢ä¸º None
        
        Args:
            value: å¾…æ¸…ç†çš„å€¼
            
        Returns:
            æ¸…ç†åçš„å€¼
        """
        try:
            if isinstance(value, float) and math.isnan(value):
                return None
        except (TypeError, ValueError):
            # isnanå¯èƒ½æŠ›å‡ºTypeErrorï¼ˆéæ•°å­—ç±»å‹ï¼‰æˆ–ValueError
            pass
        return value


    @staticmethod
    @lru_cache(maxsize=256)  # ç›´æ¥ä½¿ç”¨æ•°å€¼ï¼Œé¿å…å¾ªç¯å¼•ç”¨
    def _infer_unique_keys_from_model_cached(model_cls: Type) -> Optional[Tuple[str, ...]]:
        """ä»æ¨¡å‹æ¨æ–­å”¯ä¸€é”®çº¦æŸ - ç¼“å­˜ç‰ˆæœ¬
        
        æ¨æ–­è§„åˆ™ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰ï¼š
        1) è¡¨çº§ UniqueConstraintï¼ˆå¤åˆå”¯ä¸€ï¼‰
        2) è¡¨çº§ Index çš„å”¯ä¸€çº¦æŸï¼ˆunique=Trueï¼‰
        3) åˆ—çº§ unique=Trueï¼ˆå•åˆ—å”¯ä¸€ï¼‰
        
        Args:
            model_cls: SQLModelæ¨¡å‹ç±»
            
        Returns:
            å”¯ä¸€é”®å­—æ®µåå…ƒç»„ï¼Œè‹¥æ— åˆ™è¿”å›None
        """
        try:
            table = model_cls.__table__
            logger.debug(f"æ¨¡å‹ {model_cls.__name__} çš„è¡¨ä¿¡æ¯: ç´¢å¼•æ•°é‡={len(getattr(table, 'indexes', []))}, çº¦æŸæ•°é‡={len(getattr(table, 'constraints', []))}")
        except AttributeError as e:
            # æ¨¡å‹ç±»æ²¡æœ‰__table__å±æ€§
            logger.debug(f"æ¨¡å‹ {model_cls.__name__} æ²¡æœ‰__table__å±æ€§: {e}")
            return None

        # 1) è¡¨çº§å”¯ä¸€çº¦æŸï¼ˆå–ç¬¬ä¸€ç»„ï¼‰
        try:
            for cons in getattr(table, "constraints", set()) or set():
                try:
                    if isinstance(cons, UniqueConstraint):
                        cols = [c.name for c in cons.columns] if cons.columns else []
                        if cols:
                            return tuple(cols)
                except Exception:
                    continue
        except Exception:
            pass

        # 2) è¡¨çº§ Index çš„å”¯ä¸€çº¦æŸï¼ˆunique=Trueï¼‰
        try:
            indexes = getattr(table, "indexes", set()) or set()
            logger.debug(f"æ¨¡å‹ {model_cls.__name__} çš„ç´¢å¼•æ£€æµ‹: å…± {len(indexes)} ä¸ªç´¢å¼•")
            for idx in indexes:
                try:
                    is_unique = getattr(idx, "unique", False)
                    cols = [c.name for c in idx.columns] if hasattr(idx, 'columns') else []
                    logger.debug(f"ç´¢å¼• {getattr(idx, 'name', 'unnamed')}: unique={is_unique}, columns={cols}")
                    if isinstance(idx, Index) and is_unique and cols:
                        logger.info(f"æ‰¾åˆ°å”¯ä¸€ç´¢å¼•: {cols}")
                        return tuple(cols)
                except Exception as idx_error:
                    logger.debug(f"ç´¢å¼•æ£€æµ‹å¼‚å¸¸: {idx_error}")
                    continue
        except Exception as table_error:
            logger.debug(f"è¡¨ç´¢å¼•æ£€æµ‹å¼‚å¸¸: {table_error}")
            pass

        # 3) åˆ—çº§ unique=Trueï¼ˆå•åˆ—å”¯ä¸€ï¼‰
        try:
            for col in table.columns:
                try:
                    if getattr(col, "unique", False):
                        return (col.name,)
                except Exception:
                    continue
        except Exception:
            pass

        return None

    @staticmethod
    def _get_kline_table_model(table_type: str, year: int):
        """è·å–Kçº¿åˆ†è¡¨æ¨¡å‹
        
        Args:
            table_type: è¡¨ç±»å‹
            year: å¹´ä»½
            
        Returns:
            è¡¨æ¨¡å‹ç±»
        """
        # å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–
        from app.models.base.table_factory import TableFactory
        return TableFactory.get_table_model(table_type, year)

    @staticmethod
    def _prepare_kline_data(item: Dict[str, Any]) -> Dict[str, Any]:
        """å‡†å¤‡Kçº¿æ•°æ®ï¼šå¯¹ item ä¸­çš„å€¼è¿›è¡Œ sanitize å¤„ç†"""
        return {
            k: BatchOperations._sanitize_value(v) 
            for k, v in item.items()
        }

    @staticmethod
    def _group_items_by_year(processed_items: List[Tuple[Any, Dict[str, Any]]]) -> Dict[int, List[Dict[str, Any]]]:
        """æŒ‰å¹´ä»½å¯¹å¤„ç†åçš„æ•°æ®é¡¹è¿›è¡Œåˆ†ç»„
        
        Args:
            processed_items: (æ—¥æœŸ, æ•°æ®é¡¹)çš„å…ƒç»„åˆ—è¡¨
            
        Returns:
            æŒ‰å¹´ä»½åˆ†ç»„çš„æ•°æ®å­—å…¸
        """
        if not processed_items:
            return {}
            
        # å¯¹å¤„ç†åçš„æœ‰æ•ˆæ•°æ®æŒ‰æ—¥æœŸæ’åº
        processed_items.sort(key=lambda x: x[0])
        
        # æŒ‰å¹´ä»½åˆ†ç»„
        grouped_by_year = defaultdict(list)
        for trade_date, prepared_item in processed_items:
            grouped_by_year[trade_date.year].append(prepared_item)
            
        return dict(grouped_by_year)


    @staticmethod
    @lru_cache(maxsize=256)  # ç›´æ¥ä½¿ç”¨æ•°å€¼ï¼Œé¿å…å¾ªç¯å¼•ç”¨
    def _get_model_columns_cached(table_model: Type) -> Set[str]:
        """è·å–æ¨¡å‹çš„æ‰€æœ‰åˆ—å - ç¼“å­˜ç‰ˆæœ¬
        
        Args:
            table_model: SQLModelæ¨¡å‹ç±»
            
        Returns:
            æ¨¡å‹æ‰€æœ‰åˆ—åçš„é›†åˆ
        """
        try:
            return {c.name for c in table_model.__table__.columns}
        except AttributeError:
            # æ¨¡å‹ç±»æ²¡æœ‰__table__å±æ€§æˆ–columnså±æ€§
            return set()

    @staticmethod
    @lru_cache(maxsize=256)  # ç›´æ¥ä½¿ç”¨æ•°å€¼ï¼Œé¿å…å¾ªç¯å¼•ç”¨
    def _get_pk_names_cached(table_model: Type) -> Tuple[str, ...]:
        """è·å–æ¨¡å‹çš„ä¸»é”®åˆ—å - ç¼“å­˜ç‰ˆæœ¬
        
        Args:
            table_model: SQLModelæ¨¡å‹ç±»
            
        Returns:
            ä¸»é”®åˆ—åçš„å…ƒç»„
        """
        try:
            table = table_model.__table__
            return tuple([col.name for col in table.primary_key.columns])
        except AttributeError:
            # æ¨¡å‹ç±»æ²¡æœ‰__table__å±æ€§æˆ–primary_keyå±æ€§
            return tuple()

    @staticmethod
    @lru_cache(maxsize=512)  # ç›´æ¥ä½¿ç”¨æ•°å€¼ï¼Œé¿å…å¾ªç¯å¼•ç”¨ (LARGEç¼“å­˜ç”¨512)
    def _get_update_column_names_cached(table_model: Type, unique_keys_key: Tuple[str, ...]) -> Tuple[str, ...]:
        """è·å–å¯æ›´æ–°çš„åˆ—åé›†åˆ - ç¼“å­˜ç‰ˆæœ¬
        
        æ’é™¤ä¸å¯æ›´æ–°çš„åˆ—ï¼šå”¯ä¸€é”®ã€ä¸»é”®ã€ç³»ç»Ÿå­—æ®µ(idã€created_atã€updated_at)
        
        Args:
            table_model: SQLModelæ¨¡å‹ç±»
            unique_keys_key: å”¯ä¸€é”®åˆ—åå…ƒç»„
            
        Returns:
            å¯æ›´æ–°åˆ—åçš„å…ƒç»„
        """
        try:
            table = table_model.__table__
            table_cols = tuple(table.columns.keys())
        except AttributeError:
            # æ¨¡å‹ç±»æ²¡æœ‰__table__æˆ–columnså±æ€§
            table_cols = tuple()
        pk_names = BatchOperations._get_pk_names_cached(table_model)
        unique_set = set(unique_keys_key or tuple())
        do_not_update = unique_set | set(pk_names) | BatchOperations.NON_UPDATEABLE_FIELDS
        return tuple([c for c in table_cols if c not in do_not_update])


    @staticmethod
    def upsert_kline_partitioned(
            data: List[Dict[str, Any]],
            table_type: str,
            date_field: str = "trade_date",
            batch_size: int = 500,
    ) -> Dict[str, int]:
        """
        K çº¿åˆ†è¡¨æ‰¹é‡ upsertï¼ˆä»…é™ K çº¿ï¼šå¿…é¡»åŒ…å« ts_code/period/trade_dateï¼‰ã€‚
        
        æ³¨æ„ï¼šä¼ å…¥çš„æ•°æ®åº”è¯¥å·²ç»é€šè¿‡ä¸Šå±‚çš„ SmartDateRangeCalculator å’Œ KlinePeriodProcessor 
        è¿›è¡Œäº†æ—¥æœŸèŒƒå›´è®¡ç®—å’Œè¿‡æ»¤ï¼Œæ‰€ä»¥ç›´æ¥ä½¿ç”¨ bulk_upsert_mysql_generated å³å¯ã€‚
        MySQL çš„å”¯ä¸€é”®çº¦æŸä¼šè‡ªåŠ¨å¤„ç†é‡å¤æ•°æ®ã€‚
        
        Args:
            data: Kçº¿æ•°æ®åˆ—è¡¨ï¼ˆå·²è¿‡æ»¤ï¼ŒåªåŒ…å«éœ€è¦åŒæ­¥çš„æ•°æ®ï¼‰
            table_type: è¡¨ç±»å‹ï¼ˆå­—ç¬¦ä¸²ï¼Œå¦‚ TableTypes.STOCKï¼‰
            date_field: æ—¥æœŸå­—æ®µåï¼Œé»˜è®¤ä¸º "trade_date"
            batch_size: æ‰¹å¤„ç†å¤§å°
        """
        if not data:
            return {"inserted_count": 0, "updated_count": 0}

        # éªŒè¯è¡¨ç±»å‹ - å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–
        from app.constants.table_types import TableTypes
        if not TableTypes.is_valid_table_type(str(table_type)):
            raise ValueError(f"ä»…æ”¯æŒKçº¿åˆ†è¡¨å†™å…¥ï¼Œéæ³•çš„è¡¨ç±»å‹: {table_type}")

        # æ•°æ®é¢„å¤„ç†ï¼šå…ˆå¤„ç†æ•°æ®å¹¶è¿‡æ»¤æ— æ•ˆé¡¹ï¼Œç„¶åæŒ‰å¹´ä»½åˆ†ç»„
        # è¦æ±‚ï¼štrade_date å­—æ®µå¿…é¡»æ˜¯å­—ç¬¦ä¸²æ ¼å¼ï¼ˆYYYYMMDDï¼‰ï¼Œå¦‚ "20240101"
        
        # å…ˆå¤„ç†æ•°æ®ï¼Œè¿‡æ»¤æ— æ•ˆé¡¹
        processed_items = []
        for item in data:
            # æ£€æŸ¥å¿…è¦å­—æ®µï¼šts_codeã€periodã€date_field
            ts_code = item.get("ts_code")
            period = item.get("period")
            date_str = item.get(date_field)
            
            if not ts_code or not period or not date_str:
                logger.debug(f"è·³è¿‡æ— æ•ˆæ•°æ®é¡¹ï¼šç¼ºå°‘å¿…è¦å­—æ®µ (ts_code={ts_code}, period={period}, {date_field}={date_str})")
                continue
            
            # ğŸš€ ä¼˜åŒ–ï¼šç»Ÿä¸€æ—¥æœŸæ ¼å¼å¤„ç†
            # trade_date å¿…é¡»æ˜¯ YYYYMMDD æ ¼å¼çš„å­—ç¬¦ä¸²ï¼ˆ8ä½æ•°å­—ï¼‰
            try:
                trade_date = BatchOperations._parse_date_string(str(date_str))
                
                # ç¡®ä¿ item ä¸­åŒ…å«å¿…è¦çš„å­—æ®µï¼ˆä½¿ç”¨ item ä¸­çš„å€¼ï¼‰
                item["ts_code"] = ts_code
                item["period"] = period
                item["trade_date"] = trade_date
                
                # å¯¹ item ä¸­çš„å€¼è¿›è¡Œ sanitize å¤„ç†
                prepared_item = BatchOperations._prepare_kline_data(item)
                processed_items.append((trade_date, prepared_item))
            except (ValueError, TypeError) as e:
                logger.debug(f"æ— æ•ˆæ—¥æœŸæ ¼å¼: {date_str}, é”™è¯¯: {e}")
                continue
        
        if not processed_items:
            logger.debug("æ— å¯å†™å…¥æ•°æ®ï¼ˆè¾“å…¥é›†åˆä¸ºç©ºæˆ–æ‰€æœ‰æ•°æ®æ— æ•ˆï¼‰")
            return {"inserted_count": 0, "updated_count": 0}
        
        # ğŸš€ ä¼˜åŒ–ï¼šä½¿ç”¨è¾…åŠ©æ–¹æ³•æŒ‰å¹´ä»½åˆ†ç»„
        grouped_by_year = BatchOperations._group_items_by_year(processed_items)

        total_inserted = 0
        total_updated = 0
        processed_count = 0

        # æå‰è·å–é…ç½®ï¼Œé¿å…åœ¨å¾ªç¯ä¸­é‡å¤è·å–
        base_batch = int(batch_size or _BATCH_OPS_CONFIG.base_batch)

        for year, mappings in sorted(grouped_by_year.items()):
            if not mappings:
                continue
            
            # è·å–è¯¥å¹´ä»½çš„è¡¨æ¨¡å‹
            model = BatchOperations._get_kline_table_model(table_type, year)
            if not model:
                logger.warning(f"æœªæ‰¾åˆ° {year} å¹´çš„è¡¨æ¨¡å‹")
                continue

            try:
                # ç»Ÿä¸€ä½¿ç”¨ç”Ÿæˆå¼ upsert
                # æ³¨æ„ï¼šä¼ å…¥çš„æ•°æ®å·²ç»é€šè¿‡ä¸Šå±‚çš„ SmartDateRangeCalculator å’Œ KlinePeriodProcessor 
                # è¿›è¡Œäº†æ—¥æœŸèŒƒå›´è®¡ç®—å’Œè¿‡æ»¤ï¼ŒMySQL çš„å”¯ä¸€é”®çº¦æŸä¼šè‡ªåŠ¨å¤„ç†é‡å¤æ•°æ®
                stats = batch_operations.bulk_upsert_mysql_generated(
                    table_model=model,
                    data=mappings,
                    batch_size=base_batch,
                    enable_updated_at=True,
                ) or {"inserted": 0, "updated": 0}
                year_inserted = int(stats.get("inserted", 0))
                year_updated = int(stats.get("updated", 0))
                total_inserted += year_inserted
                total_updated += year_updated
                logger.debug(f"Kçº¿å†™å…¥ | ç±»å‹: {table_type} | å¹´ä»½: {year} | æ’å…¥: {year_inserted} | æ›´æ–°: {year_updated}")

                processed_count += len(mappings)

            except Exception as e:
                if "unknown column" in str(e).lower() and "new.id" in str(e).lower():
                    logger.error("å˜æ›´æ„ŸçŸ¥ upsert æ„é€ åŒ…å«ä¸å¯æ›´æ–°ä¸»é”®ï¼Œå·²è‡ªåŠ¨æ’é™¤ 'id' åˆ—ï¼Œè¯·é‡è¯•")
                elif "unique" in str(e).lower() or "duplicate" in str(e).lower():
                    logger.debug("éƒ¨åˆ†æ•°æ®å·²å­˜åœ¨ï¼ˆå”¯ä¸€é”®çº¦æŸï¼‰ï¼Œå·²è·³è¿‡é‡å¤")
                else:
                    logger.error(f"æ‰¹é‡å†™å…¥ {year} å¹´æ•°æ®å¤±è´¥: {e}")
                    # bulk_upsert_mysql_generated å·²å†…éƒ¨ç®¡ç†äº‹åŠ¡ï¼Œå¼‚å¸¸æ—¶ä¼šè‡ªåŠ¨ rollback
                    return {"inserted_count": total_inserted, "updated_count": total_updated}

        # bulk_upsert_mysql_generated å·²å†…éƒ¨ç®¡ç†äº‹åŠ¡å¹¶ commit
        if total_inserted > 0 or total_updated > 0:
            logger.debug(f"Kçº¿æ‰¹é‡å†™å…¥å®Œæˆ | ç±»å‹: {table_type} | æ’å…¥: {total_inserted} | æ›´æ–°: {total_updated}")

        return {"inserted_count": total_inserted, "updated_count": total_updated}

# åˆ›å»ºå…¨å±€å®ä¾‹
batch_operations = BatchOperations()
