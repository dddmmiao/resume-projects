"""
è¡Œä¸šæœåŠ¡æ¨¡å—

æä¾›è¡Œä¸šç›¸å…³çš„ä¸šåŠ¡é€»è¾‘æœåŠ¡
"""

import hashlib
from typing import Dict, List, Any, Optional

from loguru import logger

from app.core.exceptions import CancellationException
from ..core.cache_service import service_cached, cache_service
from ..external.tushare_service import tushare_service
from ...core.exceptions import ValidationException, DatabaseException
from ...dao.industry_dao import industry_dao


class IndustryService:
    """è¡Œä¸šæœåŠ¡ç±»"""

    def __init__(self):
        self.data_service = tushare_service
        self.cache_service = cache_service
        logger.info("è¡Œä¸šæœåŠ¡åˆå§‹åŒ–å®Œæˆ")

    def filter_industries(
            self,
            search: Optional[str] = None,
            limit: Optional[int] = 100,
            offset: int = 0,
            sort_by: Optional[str] = None,
            sort_period: str = "daily",
            sort_order: str = "asc",
            hot_sort: bool = False,
            ts_codes: Optional[List[str]] = None,
            trade_date: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        ç­›é€‰è¡Œä¸š

        Args:
            search: æœç´¢å…³é”®è¯
            limit: é™åˆ¶æ•°é‡
            offset: åç§»é‡
            sort_by: æ’åºå­—æ®µ
            sort_period: æ’åºå‘¨æœŸï¼ˆdaily/weekly/monthlyï¼‰
            sort_order: æ’åºæ–¹å‘
            hot_sort: æ˜¯å¦æŒ‰çƒ­åº¦æ’åº
            ts_codes: ç›´æ¥æŒ‡å®šä»£ç åˆ—è¡¨ç­›é€‰
            trade_date: äº¤æ˜“æ—¥æœŸï¼ˆYYYYMMDDæ ¼å¼ï¼‰

        Returns:
            åŒ…å«è¡Œä¸šåˆ—è¡¨å’Œæ€»æ•°çš„å­—å…¸
        """
        try:
            # å‚æ•°éªŒè¯
            if limit is not None and (limit <= 0 or limit > 1000):
                raise ValidationException("limitå‚æ•°å¿…é¡»åœ¨1-1000ä¹‹é—´")
            if offset < 0:
                raise ValidationException("offsetå‚æ•°ä¸èƒ½ä¸ºè´Ÿæ•°")

            # çƒ­åº¦æ’åºæ—¶è®¾ç½®é»˜è®¤æ’åºå­—æ®µ
            if hot_sort and not sort_by:
                sort_by = "hot_score"
                sort_order = "desc"

            # è®¾ç½®é»˜è®¤æ’åºå­—æ®µ
            if not sort_by:
                sort_by = "hot_score"
                sort_order = "desc"

            # ä½¿ç”¨æ–°çš„ç­›é€‰å™¨æ¶æ„æ„å»ºè¡Œä¸šç­›é€‰æ¡ä»¶
            filters = self._build_base_filters(ts_codes)

            # æ–°æŸ¥è¯¢æ–¹æ³•ï¼šæ ¹æ®æ’åºå­—æ®µç±»å‹é€‰æ‹©åŸºç¡€è¡¨æˆ–Kçº¿è¡¨æŸ¥è¯¢
            from ...dao.industry_dao import industry_dao
            joined = industry_dao.get_industries_smart(
                filters=filters,
                search=search,
                search_fields=["industry_name", "industry_code"],
                sort_by=sort_by or "hot_score",
                sort_period=sort_period,
                sort_order=sort_order,
                limit=limit,
                offset=offset,
                trade_date=trade_date,
            )
            # ğŸš€ ä¼˜åŒ–ï¼šç›´æ¥ä½¿ç”¨DAOæ ‡å‡†è¿”å›ï¼Œæ— éœ€æ‰‹åŠ¨è½¬æ¢
            return {"industries": joined.get("data", []), "total": joined.get("total", 0)}
        except Exception as e:
            logger.error(f"ç­›é€‰è¡Œä¸šå¤±è´¥: {str(e)}")
            raise DatabaseException(f"ç­›é€‰è¡Œä¸šå¤±è´¥: {str(e)}")

    def get_filtered_industry_codes(
            self,
            search: Optional[str] = None,
            ts_codes_filter: Optional[List[str]] = None,
            sort_by: Optional[str] = None,
            sort_order: str = "desc",
            sort_period: str = "daily",
            trade_date: Optional[str] = None,
            limit: Optional[int] = None,
    ) -> List[str]:
        """è·å–ç¬¦åˆç­›é€‰æ¡ä»¶çš„è¡Œä¸šä»£ç åˆ—è¡¨ï¼ˆæ”¯æŒæ’åºå’Œæ•°é‡é™åˆ¶ï¼‰ã€‚"""
        try:
            filters = self._build_base_filters(ts_codes_filter)
            if filters is None and ts_codes_filter:
                return []
            
            from ...dao.industry_dao import industry_dao
            return industry_dao.get_filtered_industry_codes(
                filters=filters,
                search=search,
                search_fields=["industry_name", "industry_code"],
                sort_by=sort_by,
                sort_order=sort_order,
                sort_period=sort_period,
                trade_date=trade_date,
                limit=limit,
            )
        except Exception as e:
            logger.error(f"è·å–è¡Œä¸šç­›é€‰ä»£ç åˆ—è¡¨å¤±è´¥: {str(e)}")
            return []

    @service_cached(
        "industries:stats",
        key_fn=lambda self, search=None, ts_codes=None, trade_date=None, sort_period="daily": 
            hashlib.md5(f"{trade_date or ''}:{sort_period}:{search or ''}:{','.join(sorted(ts_codes or []))}".encode()).hexdigest()[:16],
        ttl_seconds=300,
    )
    def get_industry_stats(
            self,
            search: Optional[str] = None,
            ts_codes: Optional[List[str]] = None,
            trade_date: Optional[str] = None,
            sort_period: str = "daily",
    ) -> Dict[str, Any]:
        """è·å–å½“å‰ç­›é€‰æ¡ä»¶ä¸‹çš„è¡Œä¸šæ˜ç»†æ•°æ®ï¼Œsummaryç”±å‰ç«¯è®¡ç®—ã€‚"""
        from ...dao.industry_dao import industry_dao

        try:
            filters = self._build_base_filters(ts_codes)
            stats = industry_dao.get_industry_stats_aggregated(
                filters=filters,
                search=search,
                search_fields=["industry_name", "industry_code"],
                trade_date=trade_date,
                sort_period=sort_period,
            )
            return stats
        except (ValidationException, DatabaseException):
            raise
        except Exception as e:
            logger.error(f"è·å–è¡Œä¸šç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
            raise DatabaseException(f"è·å–è¡Œä¸šç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")

    @service_cached(
        "industries:compare_stats",
        key_fn=lambda self, search=None, ts_codes=None, base_date=None, compare_date=None, sort_period="daily": 
            hashlib.md5(f"{base_date or ''}:{compare_date or ''}:{sort_period}:{search or ''}:{','.join(sorted(ts_codes or []))}".encode()).hexdigest()[:16],
        ttl_seconds=300,
    )
    def get_industry_compare_stats(
            self,
            search: Optional[str] = None,
            ts_codes: Optional[List[str]] = None,
            base_date: Optional[str] = None,
            compare_date: Optional[str] = None,
            sort_period: str = "daily",
    ) -> Dict[str, Any]:
        """è·å–ä¸¤ä¸ªæ—¥æœŸä¹‹é—´çš„è¡Œä¸šæ¶¨è·Œå¯¹æ¯”ç»Ÿè®¡ã€‚"""
        from ...dao.industry_dao import industry_dao

        try:
            filters = self._build_base_filters(ts_codes)
            stats = industry_dao.get_industry_compare_stats(
                filters=filters,
                search=search,
                search_fields=["industry_name", "industry_code"],
                base_date=base_date,
                compare_date=compare_date,
                sort_period=sort_period,
            )
            return stats
        except (ValidationException, DatabaseException):
            raise
        except Exception as e:
            logger.error(f"è·å–è¡Œä¸šå¯¹æ¯”ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
            raise DatabaseException(f"è·å–è¡Œä¸šå¯¹æ¯”ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")

    def sync_enhanced_industries(self, task_id: str = None) -> Dict[str, Any]:
        """
        åŒæ­¥è¡Œä¸šæ¿å—æ•°æ®

        Args:
            task_id: ä»»åŠ¡ID
            

        Returns:
            åŒæ­¥ç»“æœåˆ—è¡¨
        """
        try:

            # è·å–è¡Œä¸šæ•°æ®ï¼ˆDTO åˆ—è¡¨ï¼‰
            industry_dtos = self.data_service.get_industry_list(task_id=task_id)

            # ä¸¥æ ¼æ˜ å°„ï¼šDTO -> è¡Œå­—å…¸ï¼ˆä¸è¿‡æ»¤æ— ä¸Šå¸‚æ—¥æœŸçš„æ•°æ®, ä¸€èˆ¬ä¸ºå³å°†ä¸Šå¸‚çš„æ ‡çš„ï¼‰
            from ..external.tushare import mappers as strict_mappers
            if not industry_dtos:
                return {"rows": [], "total": 0}
            rows = strict_mappers.industries_to_upsert_dicts(industry_dtos)

            # ğŸš€ ä¼˜åŒ–ï¼šDAO æ‰¹é‡å†™å…¥ï¼Œç›´æ¥ä½¿ç”¨æ ‡å‡†è¿”å›
            from ...dao.industry_dao import industry_dao
            dao_result = industry_dao.bulk_upsert_industry_data(rows)

            # ç›´æ¥ä½¿ç”¨DAOæ ‡å‡†è¿”å›ï¼Œæ— éœ€æ‰‹åŠ¨è½¬æ¢
            total = dao_result.get("total_count", 0)

            # è¿”å›çœŸå®å†™åº“åçš„æ•°æ®ä¸æ€»å¤„ç†æ•°
            return {"rows": rows, "total": total}

        except CancellationException:
            raise
        except Exception as e:
            logger.error(f"åŒæ­¥è¡Œä¸šæ•°æ®å¤±è´¥: {str(e)}")
            raise DatabaseException(f"åŒæ­¥è¡Œä¸šæ•°æ®å¤±è´¥: {str(e)}")

    def sync_single_industry_stocks(
            self, industry_code: str, task_id: str = None
    ) -> int:
        """
        åŒæ­¥å•ä¸ªè¡Œä¸šçš„è‚¡ç¥¨å…³è”ï¼ˆä»APIè·å–æˆå‘˜æ•°æ®ï¼‰ã€‚

        Args:
            industry_code: è¡Œä¸šä»£ç 
            task_id: ä»»åŠ¡IDï¼ˆç”¨äºå–æ¶ˆæ£€æŸ¥ï¼‰

        Returns:
            å¤„ç†è®°å½•æ•°
        """
        try:
            if not industry_code:
                return 0

            # ä»Tushare APIè·å–è¡Œä¸šæˆå‘˜
            members = self.data_service.get_ths_member(industry_code, task_id=task_id)
            if not members:
                return 0

            # æå–è‚¡ç¥¨ä»£ç 
            stock_codes = [m.code for m in members if m.code]
            if not stock_codes:
                return 0

            # å»é‡ä¸è¿‡æ»¤ç©ºå€¼
            unique_codes = [c for c in sorted(set([c for c in stock_codes if c]))]
            if not unique_codes:
                return 0

            # æ„å»ºæ•°æ®åˆ—è¡¨
            data_list = [{"ts_code": ts_code, "industry_code": industry_code} for ts_code in unique_codes]

            # ğŸš€ ä¼˜åŒ–ï¼šæ‰¹é‡æ’å…¥æˆ–æ›´æ–°ï¼Œç›´æ¥ä½¿ç”¨DAOæ ‡å‡†è¿”å›
            from ...dao.industry_dao import industry_dao
            stats = industry_dao.bulk_upsert_stock_industry_data(data_list)
            # ç›´æ¥ä½¿ç”¨DAOæ ‡å‡†è¿”å›ï¼Œæ— éœ€æ‰‹åŠ¨è½¬æ¢
            return stats.get("total_count", 0)

        except Exception as e:
            logger.error(f"åŒæ­¥è¡Œä¸š {industry_code} è‚¡ç¥¨å…³è”å¤±è´¥: {e}")
            return 0

    def sync_industry_stock_relations(
            self,
            enhanced_industries: List[Dict[str, Any]],
            *,
            task_id: Optional[str] = None,
            optimal_workers: int = 4,
            batch_size: int = 10,
    ) -> int:
        """
        åŒæ­¥è¡Œä¸šä¸è‚¡ç¥¨çš„å…³ç³»ï¼ˆä¿æŒ scheduler åŸæœ‰æœºåˆ¶ï¼šåˆ†æ‰¹ã€å¹¶å‘ã€å–æ¶ˆæ£€æŸ¥ã€DAOè½åº“ï¼‰ã€‚

        Args:
            enhanced_industries: è¡Œä¸šåŸºç¡€æ•°æ®åˆ—è¡¨ï¼ˆéœ€åŒ…å« industry_code/industry_nameï¼‰
            task_id: ä»»åŠ¡IDï¼ˆç”¨äºå–æ¶ˆæ£€æŸ¥ï¼‰
            optimal_workers: å¹¶å‘åº¦
            batch_size: åˆ†æ‰¹å¤§å°

        Returns:
            æ–°å¢/å¤„ç†çš„å…³è”æ•°é‡ä¹‹å’Œ
        """
        try:
            if not enhanced_industries:
                return 0

            # åˆ†æ‰¹è¡Œä¸š
            industry_batches = [
                enhanced_industries[i: i + batch_size] for i in range(0, len(enhanced_industries), batch_size)
            ]

            def sync_industry_batch(industry_batch: List[Dict[str, Any]]) -> int:
                """åŒæ­¥ä¸€æ‰¹è¡Œä¸šçš„å…³ç³»ï¼ˆä½¿ç”¨æ–°çš„å•ä¸ªè¡Œä¸šåŒæ­¥æ–¹æ³•ï¼‰"""
                try:
                    total_count = 0
                    for industry_data in industry_batch:
                        ccode = industry_data.get("industry_code")
                        if not ccode:
                            continue

                        # è°ƒç”¨å•ä¸ªè¡Œä¸šåŒæ­¥æ–¹æ³•
                        count = self.sync_single_industry_stocks(ccode, task_id)
                        total_count += count

                    return total_count
                except CancellationException:
                    raise
                except Exception as e:
                    logger.error(f"è¡Œä¸šå…³ç³»æ‰¹æ¬¡åŒæ­¥å¤±è´¥: {e}")
                    return 0

            # å¹¶å‘æ‰§è¡Œæ‰¹æ¬¡
            from app.utils.concurrent_utils import process_concurrently

            results = process_concurrently(
                industry_batches,
                sync_industry_batch,
                max_workers=optimal_workers,
                error_handler=lambda batch, e: 0
            )

            total_relation_count = sum(int(result or 0) for result in results)

            return total_relation_count

        except CancellationException:
            raise
        except Exception as e:
            logger.error(f"åŒæ­¥è¡Œä¸šè‚¡ç¥¨å…³ç³»å¤±è´¥: {str(e)}")
            raise DatabaseException(f"åŒæ­¥è¡Œä¸šè‚¡ç¥¨å…³ç³»å¤±è´¥: {str(e)}")

    def cleanup_expired_data(self) -> int:
        """
        æ¸…ç†è¿‡æœŸçš„è¡Œä¸šæ•°æ®
        
        Returns:
            æ¸…ç†çš„è®°å½•æ•°
        """
        try:
            from app.models import Industry, StockIndustry
            from ...dao.query_utils import delete_records_with_filter, get_kline_table_years

            codes = self.get_all_ts_codes_cached()
            from app.services.scheduler.cleanup import compute_expired_codes
            from app.constants.table_types import TableTypes
            expired_codes = compute_expired_codes(codes, TableTypes.INDUSTRY)
            if not expired_codes:
                return 0

            years = get_kline_table_years()
            from app.services.scheduler.cleanup import cleanup_kline_for_codes
            cleanup_kline_for_codes(years, TableTypes.INDUSTRY, expired_codes)
            delete_records_with_filter(StockIndustry, StockIndustry.industry_code.in_(expired_codes))
            delete_records_with_filter(Industry, Industry.industry_code.in_(expired_codes))

            # ğŸ—‘ï¸ ç¼“å­˜å¤±æ•ˆï¼šæ¸…ç†è¿‡æœŸæ•°æ®åå¤±æ•ˆç›¸å…³ç¼“å­˜
            logger.info(f"æ¸…ç†è¿‡æœŸè¡Œä¸šæ•°æ®åï¼Œå¤±æ•ˆç›¸å…³ç¼“å­˜: {len(expired_codes)}ä¸ªä»£ç ")
            self._invalidate_caches_for_expired_codes(expired_codes)

            return len(expired_codes)

        except Exception as e:
            logger.error(f"æ¸…ç†è¿‡æœŸè¡Œä¸šæ•°æ®å¤±è´¥: {e}")
            raise DatabaseException(f"æ¸…ç†è¿‡æœŸè¡Œä¸šæ•°æ®å¤±è´¥: {str(e)}")

    def _invalidate_caches_for_expired_codes(self, expired_codes: List[str]) -> None:
        """
        ä¸ºè¿‡æœŸä»£ç å¤±æ•ˆç›¸å…³ç¼“å­˜
        
        Args:
            expired_codes: è¿‡æœŸçš„è¡Œä¸šä»£ç åˆ—è¡¨
        """
        try:
            # 1. æ¸…ç†è¡Œä¸šç›¸å…³ç¼“å­˜
            self.cache_service.invalidate_industry_cache()
            self.cache_service.invalidate_all_industry_codes()
            
            # 2. æ¸…ç†Kçº¿ç›¸å…³ç¼“å­˜
            from app.constants.table_types import TableTypes
            for period in ["daily", "weekly", "monthly"]:
                # Kçº¿æ•°æ®ç¼“å­˜
                self.cache_service.invalidate_industry_klines_for_codes(period, expired_codes)
            # æœ€æ–°æ—¥æœŸç¼“å­˜
            self.cache_service.invalidate_kline_latest_dates(TableTypes.INDUSTRY)
                
            # 3. æ¸…ç†è‚¡ç¥¨ç›¸å…³ç¼“å­˜ï¼ˆå…³è”å…³ç³»å‘ç”Ÿå˜åŒ–ï¼‰
            self.cache_service.invalidate_stock_cache()
            self.cache_service.invalidate_all_stock_codes()
            
            logger.info(f"å·²å¤±æ•ˆä¸ {len(expired_codes)} ä¸ªè¿‡æœŸè¡Œä¸šä»£ç ç›¸å…³çš„ç¼“å­˜")
        except Exception as e:
            logger.warning(f"å¤±æ•ˆç¼“å­˜æ—¶å‡ºé”™: {e}")
            # ç¼“å­˜å¤±æ•ˆå¤±è´¥ä¸åº”é˜»æ­¢æ•°æ®æ¸…ç†è¿›ç¨‹

    def search_industries(
            self,
            keyword: str,
            limit: int = 100,
    ) -> List[Dict[str, Any]]:
        """
        æœç´¢è¡Œä¸š

        Args:
            keyword: æœç´¢å…³é”®è¯
            limit: è¿”å›æ•°é‡é™åˆ¶

        Returns:
            è¡Œä¸šåˆ—è¡¨

        Raises:
            ValidationException: å‚æ•°éªŒè¯å¤±è´¥
            DatabaseException: æ•°æ®åº“æŸ¥è¯¢å¤±è´¥
        """
        try:
            # å‚æ•°éªŒè¯
            if not keyword or not keyword.strip():
                raise ValidationException("æœç´¢å…³é”®è¯ä¸èƒ½ä¸ºç©º")
            if limit <= 0 or limit > 1000:
                raise ValidationException("è¿”å›æ•°é‡é™åˆ¶å¿…é¡»åœ¨1-1000ä¹‹é—´")

            keyword = keyword.strip()
            logger.debug(f"æœç´¢è¡Œä¸š - keyword: {keyword}, limit: {limit}")

            # ä½¿ç”¨DAOæœç´¢è¡Œä¸šï¼Œåªåœ¨åç§°ä¸­æœç´¢
            industries = industry_dao.get_industries(
                search=keyword,
                search_fields=["industry_name", "industry_code"],
                limit=limit,
                offset=0
            )

            # è¿”å›ç»“æœ
            return industries

        except ValidationException:
            raise
        except Exception as e:
            logger.error(f"æœç´¢è¡Œä¸šå¤±è´¥: {str(e)}")
            raise DatabaseException(f"æœç´¢è¡Œä¸šå¤±è´¥: {str(e)}")

    # ====== æä¾›ç»™å…¶ä»–æœåŠ¡çš„è¯»æ–¹æ³•ï¼ˆå¯¹å¤–å±è”½ DAOï¼Œå†…éƒ¨ä½¿ç”¨ DAO + ç¼“å­˜ï¼‰ ======
    @service_cached("industries:members_of_stock", key_fn=lambda self, ts_code: ts_code.strip() if ts_code else "")
    def get_stock_industries_by_ts_code(self, ts_code: str) -> List[str]:
        """è¿”å›æŸè‚¡ç¥¨æ‰€å±è¡Œä¸šåç§°åˆ—è¡¨ï¼ˆæœåŠ¡å±‚è¯»ç©¿é€ç¼“å­˜ï¼‰ã€‚"""
        if not ts_code:
            return []
        return industry_dao.load_stock_industries(ts_code.strip()) or []

    def get_ts_codes_by_industry_codes(self, industry_codes: List[str]) -> List[str]:
        """æ ¹æ®è¡Œä¸šä»£ç é›†åˆè·å–å…³è”è‚¡ç¥¨ ts_code åˆ—è¡¨ï¼ˆæ— ç¼“å­˜ï¼Œç›´æ¥æŸ¥è¯¢DAOï¼‰"""
        if not industry_codes:
            return []
        try:
            return industry_dao.get_ts_codes_by_industry_codes(industry_codes)
        except Exception as e:
            logger.warning(f"è¡Œä¸šåæŸ¥è‚¡ç¥¨å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨ã€‚industry_codes={industry_codes}, error={e}")
            return []

    def get_hot_industry_codes(self) -> List[str]:
        """è·å–æ‰€æœ‰æœ‰çƒ­åº¦æ•°æ®çš„è¡Œä¸šä»£ç åˆ—è¡¨ï¼ˆæŒ‰hot_rankæ’åºï¼‰"""
        try:
            return industry_dao.get_hot_industry_codes()
        except Exception as e:
            logger.warning(f"è·å–çƒ­é—¨è¡Œä¸šä»£ç å¤±è´¥: {e}")
            return []

    from ..core.cache_service import service_cached

    @service_cached("industries:all_ts_codes", key_fn=lambda self: "v1")
    def get_all_ts_codes_cached(self) -> List[str]:
        """è·å–å…¨éƒ¨è¡Œä¸š ts_codeï¼ˆæœåŠ¡å±‚è¯»ç©¿é€ç¼“å­˜ï¼‰ã€‚"""
        try:
            from ...dao.industry_dao import industry_dao
            industries = industry_dao.get_all_ts_codes()
            # ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šå‡å°‘é‡å¤å­—å…¸è®¿é—®
            result = []
            for industry in industries:
                ts_code = industry.get("ts_code")
                if ts_code:
                    result.append(ts_code)
            return result
        except Exception:
            return []

    def _build_base_filters(self, ts_codes: Optional[List[str]]) -> Optional[Dict[str, Any]]:
        """
        æ„å»ºè¡Œä¸šç­›é€‰æ¡ä»¶
        """
        if not ts_codes:
            return None
        
        logger.info(f"ä»£ç åˆ—è¡¨ç­›é€‰(è¡Œä¸š): {len(ts_codes)}ä¸ªè¡Œä¸š")
        return {"industry_code": ts_codes}

    def get_industry_codes_by_stock_codes(self, stock_codes: List[str]) -> List[str]:
        """
        æ ¹æ®è‚¡ç¥¨ä»£ç é›†åˆè·å–å…³è”è¡Œä¸šä»£ç åˆ—è¡¨ï¼ˆæ— ç¼“å­˜ï¼Œç›´æ¥æŸ¥è¯¢DAOï¼‰
        
        Args:
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            
        Returns:
            è¡Œä¸šä»£ç åˆ—è¡¨
        """
        if not stock_codes:
            return []
        try:
            from ...dao.industry_dao import industry_dao
            return industry_dao.get_industry_codes_by_stock_codes(stock_codes)
        except Exception as e:
            logger.error(f"è·å–è‚¡ç¥¨å…³è”è¡Œä¸šä»£ç å¤±è´¥: {e}")
            return []


# åˆ›å»ºæœåŠ¡å®ä¾‹ï¼ˆå»é‡ï¼‰
industry_service = IndustryService()
