"""
è‚¡ç¥¨Kçº¿æœåŠ¡ - ä¸“é—¨å¤„ç†è‚¡ç¥¨Kçº¿æ•°æ®
"""

from typing import List, Dict, Any, Optional, Set
from datetime import datetime, timedelta

from loguru import logger

from app.constants.entity_types import EntityTypes
from app.constants.table_types import TableTypes
from app.core.exceptions import CancellationException
from app.services.scheduler.progress_utils import update_progress_with_consistent_logic
from app.utils.concurrent_utils import process_concurrently
from .base_kline_service import BaseKlineService
from ..core.cache_service import cache_service, service_cached
from ..external.tushare_service import tushare_service
from ...core.exceptions import ValidationException, DatabaseException
from ...models.schemas.kline_schemas import StockKlineItem


class StockKlineService(BaseKlineService):
    """è‚¡ç¥¨Kçº¿æ•°æ®æœåŠ¡ç±»"""

    def __init__(self):
        super().__init__(EntityTypes.STOCK)
        self.data_service = tushare_service
        logger.info("è‚¡ç¥¨Kçº¿æœåŠ¡åˆå§‹åŒ–å®Œæˆ")

    @service_cached(
        "klines:stock",
        key_fn=lambda self, ts_code, period="daily", use_cache=True: f"{period}:{ts_code}" if use_cache else "",
        ttl_seconds=86400,
    )
    def _get_stock_kline_data_full(
            self,
            ts_code: str,
            period: str = "daily",
            use_cache: bool = True,
    ) -> List[Dict[str, Any]]:
        """
        è·å–è‚¡ç¥¨Kçº¿æ•°æ®ï¼ˆå…¨é‡ï¼Œå¸¦ç¼“å­˜ï¼Œè¿”å›åŸå§‹å­—å…¸åˆ—è¡¨ï¼‰ã€‚

        Args:
            ts_code: è‚¡ç¥¨ä»£ç 
            period: å‘¨æœŸç±»å‹ (daily/weekly/monthly)

        Returns:
            Kçº¿æ•°æ®å­—å…¸åˆ—è¡¨ï¼ˆåŒ…å«æ‰€æœ‰å­—æ®µï¼ŒåŒ…æ‹¬æŒ‡æ ‡å­—æ®µï¼‰
        """
        try:
            logger.debug(f"è·å–è‚¡ç¥¨Kçº¿æ•°æ® - ts_code: {ts_code}, period: {period}")

            from ...dao.kline_query_utils import KlineQueryUtils

            # å…¨é‡å–æ•°ï¼ˆè£…é¥°å™¨å·²å¤„ç†ç¼“å­˜ä¸æ—è·¯ï¼‰
            data = KlineQueryUtils.get_kline_data(
                ts_code=ts_code,
                period=period,
                table_type=TableTypes.STOCK,
            )
            return data

        except Exception as e:
            logger.error(f"è·å–è‚¡ç¥¨Kçº¿æ•°æ®å¤±è´¥: {str(e)}")
            raise DatabaseException(f"è·å–è‚¡ç¥¨Kçº¿æ•°æ®å¤±è´¥: {str(e)}")

    def get_stock_kline_data(
            self,
            ts_code: str,
            period: str = "daily",
            limit: int = 500,
            end_date: Optional[str] = None,
    ) -> List[StockKlineItem]:
        """
        è·å–è‚¡ç¥¨Kçº¿æ•°æ®ï¼ˆæŒ‰ limit åˆ‡ç‰‡ï¼Œè½¬æ¢ä¸ºPydanticæ¨¡å‹ï¼Œè¿‡æ»¤æŒ‡æ ‡å­—æ®µï¼‰ã€‚

        Args:
            ts_code: è‚¡ç¥¨ä»£ç 
            period: å‘¨æœŸç±»å‹ (daily/weekly/monthly)
            limit: é™åˆ¶æ•°é‡
            end_date: ç»“æŸæ—¥æœŸ (YYYYMMDDæ ¼å¼)ï¼ŒKçº¿æ•°æ®æˆªæ­¢åˆ°è¯¥æ—¥æœŸ

        Returns:
            Kçº¿æ•°æ®åˆ—è¡¨ï¼ˆPydanticæ¨¡å‹ï¼Œä¸åŒ…å«æŒ‡æ ‡å­—æ®µï¼‰
        """
        # æ ¹æ®ç³»ç»Ÿé…ç½®çš„æœ€å¤§æ˜¾ç¤ºå¹´ä»½ï¼Œæ ¡éªŒlimit
        from ...dao.query_config import QueryConfig
        effective_limit = QueryConfig.get_effective_limit(limit)
        
        # è·å–å®Œæ•´æ•°æ®ï¼ˆåŒ…å«æŒ‡æ ‡å­—æ®µï¼Œä½¿ç”¨ç¼“å­˜ï¼‰
        data = self._get_stock_kline_data_full(ts_code=ts_code, period=period)
        
        # å¦‚æœæŒ‡å®šäº†ç»“æŸæ—¥æœŸï¼Œè¿‡æ»¤æ•°æ®
        # å‰ç«¯è´Ÿè´£å°†å‘¨çº¿/æœˆçº¿çš„æ—¥æœŸè½¬æ¢ä¸ºå¯¹åº”å‘¨æœŸçš„ç»“æŸæ—¥æœŸ
        # æ³¨ï¼štrade_date å·²åœ¨ _process_kline_row ä¸­è½¬ä¸º YYYYMMDD æ ¼å¼
        if end_date and data:
            data = [item for item in data if item.get('trade_date', '') <= end_date]
        
        # é™åˆ¶æ•°é‡ï¼ˆä½¿ç”¨æ ¡éªŒåçš„effective_limitï¼‰
        if effective_limit and len(data) > effective_limit:
            data = data[-effective_limit:]
        
        # è½¬æ¢ä¸ºPydanticæ¨¡å‹ï¼ˆè‡ªåŠ¨è¿‡æ»¤æœªå®šä¹‰çš„å­—æ®µï¼Œå³æŒ‡æ ‡å­—æ®µï¼‰
        from ...dao.kline_query_utils import KlineQueryUtils
        from ...constants.table_types import TableTypes
        
        return KlineQueryUtils.convert_kline_data_to_models(data, TableTypes.STOCK)

    # ============== æŒ‡æ ‡æ•°æ®ï¼ˆç›´æ¥ä½¿ç”¨Kçº¿æ•°æ®ç¼“å­˜ï¼Œé¿å…é‡å¤ç¼“å­˜ï¼‰ ==============
    def _get_stock_indicators_full(self, ts_code: str, period: str = "daily") -> List[Dict[str, Any]]:
        """
        è·å–è‚¡ç¥¨æŒ‡æ ‡æ•°æ®ï¼ˆç›´æ¥ä½¿ç”¨Kçº¿æ•°æ®ç¼“å­˜ï¼Œå› ä¸ºKçº¿æ•°æ®å·²åŒ…å«æ‰€æœ‰æŒ‡æ ‡å­—æ®µï¼‰
        
        Args:
            ts_code: è‚¡ç¥¨ä»£ç 
            period: å‘¨æœŸç±»å‹
            
        Returns:
            æŒ‡æ ‡æ•°æ®åˆ—è¡¨ï¼ˆå®é™…å°±æ˜¯Kçº¿æ•°æ®ï¼ŒåŒ…å«æ‰€æœ‰æŒ‡æ ‡å­—æ®µï¼‰
        """
        try:
            # ç›´æ¥ä½¿ç”¨Kçº¿æ•°æ®ç¼“å­˜ï¼ŒKçº¿æ•°æ®å·²åŒ…å«æ‰€æœ‰æŒ‡æ ‡å­—æ®µ
            data = self._get_stock_kline_data_full(ts_code=ts_code, period=period)
            if isinstance(data, list):
                return data
            return []
        except Exception as e:
            logger.error(f"è·å–è‚¡ç¥¨æŒ‡æ ‡æ•°æ®å¤±è´¥: ts_code={ts_code}, period={period}, error={e}")
            return []

    def get_stock_indicators_cached(self, ts_code: str, period: str = "daily", limit: int = 100, end_date: Optional[str] = None) -> List[
        Dict[str, Any]]:
        data = self._get_stock_indicators_full(ts_code=ts_code, period=period)
        if not isinstance(data, list):
            return []
        # æŒ‰end_dateè¿‡æ»¤
        if end_date:
            data = [d for d in data if d.get('trade_date', '') <= end_date]
        # æŒ‰limitæˆªæ–­
        if limit and len(data) > int(limit):
            return data[-int(limit):]
        return data

    def batch_get_stock_indicators_cached(self, ts_codes: List[str], period: str = "daily", limit: int = 100, end_date: Optional[str] = None) -> Dict[
        str, List[Dict[str, Any]]]:
        """æ‰¹é‡è·å–è‚¡ç¥¨æŒ‡æ ‡æ•°æ®ï¼ˆåŸºäºåŸæœ‰ç¼“å­˜æ–¹æ¡ˆä¼˜åŒ–ï¼‰"""
        if not ts_codes:
            return {}

        from app.utils.concurrent_utils import process_concurrently, ConcurrentConfig

        def fetch_single(code: str):
            try:
                data = self.get_stock_indicators_cached(ts_code=code, period=period, limit=limit, end_date=end_date)
                return (code, data or [])
            except Exception as e:
                logger.debug(f"è·å–è‚¡ç¥¨æŒ‡æ ‡æ•°æ®å¤±è´¥: {code}, {e}")
                return (code, [])

        max_workers = ConcurrentConfig.get_optimal_workers()
        results = process_concurrently(ts_codes, fetch_single, max_workers=max_workers)

        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        result = {code: data for code, data in results if data}

        return result

    def sync_stock_kline_data(
            self,
            ts_codes: List[str],
            periods=None,
            force_sync: bool = False,
            concurrent_workers: int = 0,
            task_id: str = None,
            start_date: Optional[str] = None,
            end_date: Optional[str] = None,
    ) -> Dict[str, int]:
        """
        åŒæ­¥è‚¡ç¥¨Kçº¿æ•°æ®

        Args:
            ts_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            periods: å‘¨æœŸç±»å‹åˆ—è¡¨
            force_sync: æ˜¯å¦å¼ºåˆ¶åŒæ­¥
            concurrent_workers: å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°
            task_id: ä»»åŠ¡IDï¼Œç”¨äºå–æ¶ˆæ£€æŸ¥

        Returns:
            { "inserted_count": æ–°å¢æ¡æ•°, "updated_count": æ›´æ–°æ¡æ•° }
        """
        return self.sync_kline_data_universal(
            ts_codes=ts_codes,
            periods=periods,
            force_sync=force_sync,
            concurrent_workers=concurrent_workers,
            task_id=task_id,
            start_date=start_date,
            end_date=end_date,
        )

    def _fetch_kline_from_tushare(
            self, ts_code: str, start_date: str, end_date: str, task_id: str = None
    ) -> List[Dict[str, Any]]:
        """ä»Tushareè·å–è‚¡ç¥¨Kçº¿æ•°æ®ï¼ˆå«æ¯æ—¥æŒ‡æ ‡ï¼‰"""
        import time
        start_time = time.time()
        
        try:
            # ä½¿ç”¨è‚¡ç¥¨æ¥å£è·å–Kçº¿æ•°æ®
            logger.debug(f"å¼€å§‹è·å– {ts_code} çš„Kçº¿æ•°æ®ï¼Œæ—¥æœŸèŒƒå›´: {start_date}-{end_date}")
            
            api_start_time = time.time()
            daily_dtos = self.data_service.get_daily_data(
                ts_code=ts_code, start_date=start_date, end_date=end_date, task_id=task_id
            )
            api_duration = time.time() - api_start_time

            # è½¬æ¢ DTO ä¸ºå¯å˜å­—å…¸åˆ—è¡¨
            from ..external.tushare import mappers as strict_mappers
            kline_data = strict_mappers.stock_kline_to_upsert_dicts(daily_dtos) if daily_dtos else []

            if not kline_data:
                total_duration = time.time() - start_time
                logger.warning(f"æœªè·å–åˆ°Kçº¿æ•°æ® | ts_code: {ts_code} | APIè€—æ—¶: {api_duration:.3f}s | æ€»è€—æ—¶: {total_duration:.3f}s")
                return []

            # è·å–æ¯æ—¥æŒ‡æ ‡æ•°æ®å¹¶åˆå¹¶åˆ°Kçº¿æ•°æ®ä¸­
            try:
                daily_basic_dtos = self.data_service.get_daily_basic(
                    ts_code=ts_code, start_date=start_date, end_date=end_date, task_id=task_id
                )
                if daily_basic_dtos:
                    # æ„å»ºæ—¥æœŸåˆ°æŒ‡æ ‡æ•°æ®çš„æ˜ å°„
                    basic_map = {dto.trade_date: dto for dto in daily_basic_dtos}
                    # åˆå¹¶æŒ‡æ ‡æ•°æ®åˆ°Kçº¿æ•°æ®
                    for kline in kline_data:
                        trade_date = kline.get('trade_date')
                        if trade_date:
                            # è½¬æ¢æ—¥æœŸæ ¼å¼ï¼ˆå¦‚æœéœ€è¦ï¼‰
                            date_key = trade_date.replace('-', '') if '-' in str(trade_date) else str(trade_date)
                            basic = basic_map.get(date_key)
                            if basic:
                                kline['turnover_rate_f'] = basic.turnover_rate_f
                                kline['volume_ratio'] = basic.volume_ratio
                                kline['pe'] = basic.pe
                                kline['pe_ttm'] = basic.pe_ttm
                                kline['pb'] = basic.pb
                                kline['ps'] = basic.ps
                                kline['ps_ttm'] = basic.ps_ttm
                                kline['dv_ratio'] = basic.dv_ratio
                                kline['dv_ttm'] = basic.dv_ttm
                                kline['total_share'] = basic.total_share
                                kline['float_share'] = basic.float_share
                                kline['free_share'] = basic.free_share
                                kline['total_mv'] = basic.total_mv
                                kline['circ_mv'] = basic.circ_mv
                    logger.debug(f"åˆå¹¶æ¯æ—¥æŒ‡æ ‡ | ts_code: {ts_code} | æŒ‡æ ‡è®°å½•: {len(daily_basic_dtos)}")
            except Exception as e:
                logger.warning(f"è·å–æ¯æ—¥æŒ‡æ ‡å¤±è´¥ | ts_code: {ts_code} | é”™è¯¯: {e}")

            total_duration = time.time() - start_time
            logger.debug(
                f"è·å–è‚¡ç¥¨Kçº¿ | ts_code: {ts_code} | "
                f"è®°å½•: {len(kline_data)} | "
                f"è€—æ—¶: {total_duration:.2f}s"
            )
            return kline_data

        except CancellationException:
            # é‡æ–°æŠ›å‡ºå–æ¶ˆå¼‚å¸¸ï¼Œè®©ä¸Šå±‚å¤„ç†
            raise
        except Exception as e:
            total_duration = time.time() - start_time
            logger.error(f"âŒ è·å–è‚¡ç¥¨Kçº¿å¤±è´¥ | ts_code: {ts_code} | è€—æ—¶: {total_duration:.3f}s | é”™è¯¯: {e}")
            logger.error(f"é”™è¯¯è¯¦æƒ…: ts_code={ts_code}, start_date={start_date}, end_date={end_date}")
            return []

    def _bulk_store_data(
            self,
            data: List[Dict[str, Any]],
            batch_size: int = 500
    ) -> Dict[str, int]:
        """æ‰¹é‡å­˜å‚¨è‚¡ç¥¨Kçº¿æ•°æ®"""
        import time
        start_time = time.time()
        
        try:
            logger.debug(f"å¼€å§‹æ‰¹é‡å­˜å‚¨è‚¡ç¥¨Kçº¿ | æ•°é‡: {len(data)} | batch_size: {batch_size}")
            from ...dao.stock_kline_dao import stock_kline_dao

            result = stock_kline_dao.bulk_upsert_stock_kline_data(
                data=data,
                batch_size=batch_size
            )

            total_duration = time.time() - start_time
            total_changed = result.get("inserted_count", 0) + result.get("updated_count", 0)

            # ä¼˜åŒ–æ—¥å¿—ï¼šä¿ç•™å…³é”®ç»Ÿè®¡ä¿¡æ¯
            if total_changed > 0:
                logger.info(
                    f"è‚¡ç¥¨Kçº¿æ•°æ®æ›´æ–° | "
                    f"æ’å…¥: {result.get('inserted_count', 0)} | "
                    f"æ›´æ–°: {result.get('updated_count', 0)} | "
                    f"æ€»è®¡: {total_changed} | "
                    f"è€—æ—¶: {total_duration:.2f}ç§’"
                )
            else:
                logger.debug(
                    f"è‚¡ç¥¨Kçº¿æ•°æ®å·²æ˜¯æœ€æ–° | "
                    f"æ•°æ®é‡: {len(data)}æ¡ | "
                    f"è€—æ—¶: {total_duration:.2f}ç§’"
                )

            return result

        except Exception as e:
            total_duration = time.time() - start_time
            logger.error(f"âŒ è‚¡ç¥¨Kçº¿å­˜å‚¨å¤±è´¥ | è€—æ—¶: {total_duration:.3f}s | é”™è¯¯: {e}")
            return {"inserted_count": 0, "updated_count": 0}

    def _invalidate_cache(self, period: str, codes: List[str]):
        """å¤±æ•ˆè‚¡ç¥¨ç¼“å­˜"""
        cache_service.invalidate_stock_klines_for_codes(period, codes)

    def _get_kline_data_full_method(self, period: str):
        """è·å–å¸¦ç¼“å­˜çš„Kçº¿æ•°æ®æ–¹æ³•ï¼ˆç”¨äºé¢„çƒ­ï¼‰"""
        return lambda code: self._get_stock_kline_data_full(code, period)

    def sync_auction_data(
            self,
            force_sync: bool = False,
            task_id: str = None,
            ts_codes: Optional[List[str]] = None,
            start_date: Optional[str] = None,
            end_date: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        åŒæ­¥å¼€ç›˜ç«ä»·æ•°æ®ï¼ˆæ”¯æŒå…¨é‡å’Œå¢é‡åŒæ­¥ï¼‰
        
        æ ¸å¿ƒèŒè´£ï¼š
        - è®¡ç®—æ—¥æœŸèŒƒå›´
        - è·å–å¼€ç›˜ç«ä»·æ•°æ®
        - å¤„ç†å¹¶å­˜å‚¨æ•°æ®
        - è¿”å›åŒæ­¥ç»“æœå’Œæ‰€æœ‰è‚¡ç¥¨ä»£ç 
        
        å‰¯ä½œç”¨ï¼ˆå¦‚ç¼“å­˜å¤±æ•ˆï¼‰ç”±è°ƒç”¨æ–¹è´Ÿè´£ã€‚
        
        Args:
            force_sync: æ˜¯å¦å¼ºåˆ¶åŒæ­¥ï¼ˆå…¨é‡ï¼‰ï¼Œå¦‚æœä¸ºFalseåˆ™å¢é‡åŒæ­¥
            task_id: ä»»åŠ¡IDï¼Œç”¨äºå–æ¶ˆæ£€æŸ¥
            ts_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™åŒæ­¥æ‰€æœ‰è‚¡ç¥¨
            start_date: æ˜¾å¼å¼€å§‹æ—¥æœŸï¼ˆYYYYMMDDï¼‰ï¼Œä¼˜å…ˆä½¿ç”¨æ­¤æ—¥æœŸè€Œä¸æ˜¯æ™ºèƒ½è®¡ç®—
            end_date: æ˜¾å¼ç»“æŸæ—¥æœŸï¼ˆYYYYMMDDï¼‰ï¼Œä¼˜å…ˆä½¿ç”¨æ­¤æ—¥æœŸè€Œä¸æ˜¯æ™ºèƒ½è®¡ç®—
            
        Returns:
            {
                "inserted_count": æ–°å¢æ¡æ•°,
                "updated_count": æ›´æ–°æ¡æ•°,
                "total": æ€»æ¡æ•°,
                "ts_codes": æ‰€æœ‰è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼ˆç”¨äºç¼“å­˜å¤±æ•ˆï¼‰
            }
        """
        import time
        from ...services.data.stock_service import stock_service
        from ..core.smart_date_range_calculator import SmartDateRangeCalculator
        
        start_time = time.time()
        
        try:
            # è·å–è‚¡ç¥¨ä»£ç åˆ—è¡¨
            if ts_codes is None:
                # å¦‚æœæ²¡æœ‰æŒ‡å®šcodesï¼Œåˆ™è·å–æ‰€æœ‰è‚¡ç¥¨ä»£ç 
                ts_codes = stock_service.get_all_ts_codes_cached()
                if not ts_codes:
                    logger.warning("æœªè·å–åˆ°è‚¡ç¥¨ä»£ç åˆ—è¡¨")
                    return {"inserted_count": 0, "updated_count": 0, "total": 0, "ts_codes": []}
                logger.info(f"è·å–åˆ° {len(ts_codes)} åªè‚¡ç¥¨ï¼ˆå…¨éƒ¨ï¼‰")
            else:
                # å¦‚æœæŒ‡å®šäº†codesï¼Œä½¿ç”¨æŒ‡å®šçš„codes
                if not ts_codes:
                    logger.warning("è‚¡ç¥¨ä»£ç åˆ—è¡¨ä¸ºç©º")
                    return {"inserted_count": 0, "updated_count": 0, "total": 0, "ts_codes": []}
                logger.info(f"åŒæ­¥æŒ‡å®š {len(ts_codes)} åªè‚¡ç¥¨çš„ç«ä»·æ•°æ®")
            
            # å§‹ç»ˆè°ƒç”¨æ™ºèƒ½è®¡ç®—å™¨è·å– latest_daily_datesï¼ˆç”¨äºåŒºåˆ†æ–°/æ—§è®°å½•ï¼‰
            # æ˜¾å¼æ—¥æœŸæ¨¡å¼ = æŒ‡å®šèŒƒå›´çš„å…¨é‡åŒæ­¥ï¼Œforce_sync=True ç¡®ä¿æ‰€æœ‰ä»£ç éƒ½æœ‰æ—¥æœŸèŒƒå›´
            period_ranges = SmartDateRangeCalculator.calculate_period_ranges_for_codes(
                ts_codes=ts_codes,
                periods=["daily"],
                entity_type=EntityTypes.STOCK,
                force_sync=True,  # å§‹ç»ˆä½¿ç”¨å…¨é‡æ¨¡å¼è®¡ç®—ï¼Œç¡®ä¿æ‰€æœ‰ä»£ç éƒ½æœ‰èŒƒå›´
            )

            # è®¡ç®—æ¯ä¸ªä»£ç å¯¹åº”çš„æœ€æ–°æ—¥çº¿æ—¥æœŸï¼ˆç”¨äºåŒºåˆ†æ–°æ—§è®°å½•ï¼‰
            latest_daily_dates: Dict[str, str] = {
                code: ranges.get("_latest_daily")
                for code, ranges in period_ranges.items()
                if ranges.get("_latest_daily")
            }

            # æ ¹æ®æ˜¯å¦æœ‰æ˜¾å¼æ—¥æœŸå†³å®š date_ranges çš„æ¥æº
            if start_date and end_date:
                # æ˜¾å¼æ—¥æœŸæ¨¡å¼ï¼šä½¿ç”¨æŒ‡å®šçš„æ—¥æœŸèŒƒå›´
                logger.info(f"ä½¿ç”¨æ˜¾å¼æ—¥æœŸèŒƒå›´åŒæ­¥ç«ä»·æ•°æ®: {start_date} åˆ° {end_date}")
                date_ranges = {code: (start_date, end_date) for code in ts_codes}
            else:
                # æ™ºèƒ½è®¡ç®—æ¨¡å¼ï¼šä» period_ranges æå– daily èŒƒå›´
                date_ranges = {
                    code: ranges["daily"]
                    for code, ranges in period_ranges.items()
                    if ranges.get("daily")
                }

            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ•°æ®éƒ½æ˜¯æœ€æ–°çš„
            if not date_ranges:
                logger.info(
                    f"ğŸ¯ æ‰€æœ‰å¼€ç›˜ç«ä»·æ•°æ®éƒ½æ˜¯æœ€æ–°çš„ï¼Œæ— éœ€åŒæ­¥ | "
                    f"æ€»ä»£ç æ•°: {len(ts_codes)}"
                )
                return {"inserted_count": 0, "updated_count": 0, "total": 0, "ts_codes": ts_codes}

            # è®¡ç®—ç»Ÿä¸€çš„æ—¥æœŸèŒƒå›´ï¼ˆå…¨å±€ start/endï¼‰
            if start_date and end_date:
                # æ˜¾å¼æ—¥æœŸæ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨æä¾›çš„æ—¥æœŸèŒƒå›´
                global_start_date = start_date
                global_end_date = end_date
            elif force_sync:
                # å…¨é‡æ¨¡å¼ï¼šä»ç„¶ä½¿ç”¨å„ä»£ç  daily èŒƒå›´çš„æœ€å°/æœ€å¤§å€¼
                all_ranges = list(date_ranges.values())
                global_start_date = min(r[0] for r in all_ranges)
                global_end_date = max(r[1] for r in all_ranges)
            else:
                # å¢é‡æ¨¡å¼ï¼šç«ä»·åªæ‹‰å½“å¤©è¿™ä¸€æ ¹ï¼Œä¸è¡¥å†å²ç¼ºå£
                today_str = datetime.now().strftime("%Y%m%d")
                global_start_date = today_str
                global_end_date = today_str

            codes_to_sync_count = len(date_ranges)
            codes_up_to_date = len(ts_codes) - codes_to_sync_count
            sync_mode = "å…¨é‡" if force_sync else "å¢é‡"
            logger.info(
                f"ğŸš€ å¼€å§‹{sync_mode}åŒæ­¥å¼€ç›˜ç«ä»·æ•°æ® | "
                f"æ€»ä»£ç æ•°: {len(ts_codes)}, "
                f"éœ€è¦åŒæ­¥: {codes_to_sync_count}, "
                f"å·²æ˜¯æœ€æ–°: {codes_up_to_date}, "
                f"æ—¥æœŸèŒƒå›´: {global_start_date} åˆ° {global_end_date}"
            )

            # æŒ‰æ¨¡å¼é€‰æ‹©å–æ•°æ–¹å¼ï¼š
            # - å¢é‡ï¼šæŒ‰äº¤æ˜“æ—¥å¾ªç¯ï¼Œä½¿ç”¨ trade_date å‚æ•°æ‰¹é‡è·å–æ‰€æœ‰è‚¡ç¥¨çš„ç«ä»·æ•°æ®
            # - å…¨é‡ï¼šåœ¨ worker ä¸­æŒ‰è‚¡ç¥¨ä»£ç  + æ—¥æœŸåŒºé—´è°ƒç”¨ Tushare
            trade_dates: List[str] = []

            if not force_sync:
                # æŒ‰äº¤æ˜“æ—¥ä» Tushare è·å–ç«ä»·æ•°æ®ï¼ˆæ¯ä¸ªäº¤æ˜“æ—¥ä¸€æ¬¡è°ƒç”¨ï¼‰ï¼Œç”± worker è´Ÿè´£å®é™…æ‹‰å–ä¸å¤„ç†
                start_dt_obj = datetime.strptime(global_start_date, "%Y%m%d").date()
                end_dt_obj = datetime.strptime(global_end_date, "%Y%m%d").date()
                total_days = (end_dt_obj - start_dt_obj).days + 1

                logger.info(
                    f"æŒ‰äº¤æ˜“æ—¥ä» Tushare è·å–ç«ä»·æ•°æ® | "
                    f"æ—¥æœŸèŒƒå›´: {global_start_date} åˆ° {global_end_date}ï¼Œé¢„è®¡å¤©æ•°: {total_days}"
                )

                cur_date = start_dt_obj
                while cur_date <= end_dt_obj:
                    trade_dates.append(cur_date.strftime("%Y%m%d"))
                    cur_date = cur_date + timedelta(days=1)
            else:
                logger.info(
                    "æŒ‰ code ä» Tushare è·å–ç«ä»·æ•°æ®"
                )

            # ä¸ K çº¿åŒæ­¥ä¿æŒä¸€è‡´ï¼šæ¯ä¸ª worker è´Ÿè´£å•ä¸ªå•ä½çš„æœ¬åœ°å¤„ç† + å…¥åº“
            total_codes = len(date_ranges)

            logger.info(
                f"æŒ‰è‚¡ç¥¨ä»£ç å¤„ç†ï¼ˆå¹¶å‘ï¼‰ | "
                f"è‚¡ç¥¨æ•°é‡: {total_codes}, "
                f"æ—¥æœŸèŒƒå›´: {global_start_date} åˆ° {global_end_date}"
            )

            # é€‚åº¦å¹¶å‘ï¼Œé¿å…å¯¹ Tushare å’Œæ•°æ®åº“å‹åŠ›è¿‡å¤§
            max_workers = 6

            from .auction_data_processor import auction_data_processor

            def worker_incremental(trade_date: str) -> Dict[str, Any]:
                """å¢é‡æ¨¡å¼ï¼šæŒ‰äº¤æ˜“æ—¥ä» Tushare è·å–ç«ä»·æ•°æ®åè¿›è¡Œæœ¬åœ°å¤„ç† + å…¥åº“"""
                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å–æ¶ˆ
                if task_id:
                    from app.services.core.redis_task_manager import redis_task_manager
                    task_info = redis_task_manager.get_task_progress(task_id)
                    if task_info and task_info.get("status") == "cancelling":
                        raise CancellationException("ä»»åŠ¡å·²å–æ¶ˆ")

                worker_start = time.time()
                # æŒ‰äº¤æ˜“æ—¥ä» Tushare è·å–å½“æ—¥æ‰€æœ‰è‚¡ç¥¨çš„ç«ä»·æ•°æ®
                fetch_start = time.time()
                day_dtos = self.data_service.get_auction_data(
                    trade_date=trade_date,
                    task_id=task_id,
                )
                fetch_duration = time.time() - fetch_start

                # æ ¹æ®æ¯åªè‚¡ç¥¨çš„æ—¥æœŸèŒƒå›´è¿›è¡Œè¿‡æ»¤
                # åªå¤„ç†åœ¨ date_ranges ä¸­æŒ‡å®šçš„è‚¡ç¥¨ï¼Œé¿å…åŒæ­¥ç”¨æˆ·æœªè¯·æ±‚çš„è‚¡ç¥¨
                filtered_dtos: List[Any] = []
                codes_for_day: Set[str] = set()

                for dto in day_dtos:
                    code = dto.ts_code
                    if not code:
                        continue
                    # åªå¤„ç†åœ¨ date_ranges ä¸­å­˜åœ¨çš„è‚¡ç¥¨
                    if code not in date_ranges:
                        continue
                    filtered_dtos.append(dto)
                    codes_for_day.add(code)

                if not day_dtos:
                    logger.debug(
                        f"âš ï¸ æœªè·å–åˆ° {trade_date} çš„ç«ä»·æ•°æ®ï¼Œ"
                        f"APIè€—æ—¶: {fetch_duration:.3f}ç§’"
                    )

                if not filtered_dtos:
                    logger.debug(
                        f"âš ï¸ {trade_date} çš„ç«ä»·æ•°æ®ä¸­æ²¡æœ‰éœ€è¦åœ¨æ—¥æœŸèŒƒå›´å†…åŒæ­¥çš„è®°å½•"
                    )

                # æœ¬åœ°å¤„ç†å¹¶å…¥åº“
                process_start = time.time()
                result = auction_data_processor.process_auction_data(
                    auction_dtos=filtered_dtos,
                    bulk_store_func=lambda data, batch: self._bulk_store_data(data, batch),
                    batch_size=500,
                    latest_daily_dates=latest_daily_dates,
                )
                process_duration = time.time() - process_start

                inserted = int(result.get("inserted_count", 0) or 0)
                updated = int(result.get("updated_count", 0) or 0)
                total_duration = time.time() - worker_start

                logger.debug(
                    f"{trade_date} ç«ä»·æ•°æ®åŒæ­¥å®Œæˆ | "
                    f"æ¶‰åŠè‚¡ç¥¨æ•°: {len(codes_for_day)} | "
                    f"è¿‡æ»¤åæ¡æ•°: {len(filtered_dtos)} | "
                    f"æ’å…¥: {inserted}, æ›´æ–°: {updated} | "
                    f"æ€»è€—æ—¶: {total_duration:.2f}ç§’ | "
                    f"Tushare: {fetch_duration:.2f}ç§’ | "
                    f"æœ¬åœ°å¤„ç†+å…¥åº“: {process_duration:.2f}ç§’"
                )

                return {
                    "trade_date": trade_date,
                    "codes": list(codes_for_day),
                    "count": len(filtered_dtos),
                    "inserted": inserted,
                    "updated": updated,
                    "error": False,
                }

            def worker_full(code: str) -> Dict[str, Any]:
                """å…¨é‡æ¨¡å¼ï¼šæŒ‰è‚¡ç¥¨ä»£ç  + æ—¥æœŸåŒºé—´ä» Tushare è·å– DTO åè¿›è¡Œæœ¬åœ°å¤„ç† + å…¥åº“"""
                # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å–æ¶ˆ
                if task_id:
                    from app.services.core.redis_task_manager import redis_task_manager
                    task_info = redis_task_manager.get_task_progress(task_id)
                    if task_info and task_info.get("status") == "cancelling":
                        raise CancellationException("ä»»åŠ¡å·²å–æ¶ˆ")

                worker_start = time.time()

                code_range = date_ranges.get(code)
                if not code_range:
                    logger.warning(f"å…¨é‡åŒæ­¥æ—¶æœªæ‰¾åˆ°æ—¥æœŸèŒƒå›´ | ts_code: {code}")
                    return {
                        "code": code,
                        "count": 0,
                        "inserted": 0,
                        "updated": 0,
                        "error": False,
                    }

                start_i, end_i = code_range
                fetch_start = time.time()
                code_dtos = self.data_service.get_auction_data(
                    ts_code=code,
                    start_date=start_i,
                    end_date=end_i,
                    task_id=task_id,
                )
                fetch_duration = time.time() - fetch_start

                if not code_dtos:
                    logger.warning(
                        f"âš ï¸ å…¨é‡åŒæ­¥ {code} çš„ç«ä»·æ•°æ®ä¸ºç©ºï¼ˆèŒƒå›´: {start_i}..{end_i}ï¼‰ï¼Œ"
                        f"APIè€—æ—¶: {fetch_duration:.3f}ç§’"
                    )
                    return {
                        "code": code,
                        "count": 0,
                        "inserted": 0,
                        "updated": 0,
                        "error": False,
                    }

                # æœ¬åœ°å¤„ç†å¹¶å…¥åº“
                process_start = time.time()
                result = auction_data_processor.process_auction_data(
                    auction_dtos=code_dtos,
                    bulk_store_func=lambda data, batch: self._bulk_store_data(data, batch),
                    batch_size=500,
                    latest_daily_dates=latest_daily_dates,
                )
                process_duration = time.time() - process_start

                inserted = int(result.get("inserted_count", 0) or 0)
                updated = int(result.get("updated_count", 0) or 0)
                total_duration = time.time() - worker_start

                logger.debug(
                    f"{code} ç«ä»·æ•°æ®åŒæ­¥å®Œæˆ | "
                    f"æ—¥æœŸèŒƒå›´: {start_i}..{end_i} | "
                    f"æ‹‰å–æ¡æ•°: {len(code_dtos)} | "
                    f"æ’å…¥: {inserted}, æ›´æ–°: {updated} | "
                    f"æ€»è€—æ—¶: {total_duration:.2f}ç§’ | "
                    f"æœ¬åœ°å¤„ç†+å…¥åº“: {process_duration:.2f}ç§’"
                )

                return {
                    "code": code,
                    "count": len(code_dtos),
                    "inserted": inserted,
                    "updated": updated,
                    "error": False,
                }

            total_records = 0
            total_inserted = 0
            total_updated = 0

            def progress_callback(result: Dict[str, Any], completed: int, total: int) -> None:
                """è¿›åº¦å›è°ƒï¼šè®¡ç®— ETAã€è¾“å‡ºæ—¥å¿—å¹¶æ›´æ–°ä»»åŠ¡è¿›åº¦"""
                nonlocal total_records, total_inserted, total_updated

                if result:
                    count = int(result.get("count", 0) or 0)
                    inserted = int(result.get("inserted", 0) or 0)
                    updated = int(result.get("updated", 0) or 0)
                else:
                    count = inserted = updated = 0

                total_records += count
                total_inserted += inserted
                total_updated += updated

                elapsed = time.time() - start_time
                avg_time = elapsed / completed if completed > 0 else 0.0
                remaining = (total - completed) * avg_time

                logger.info(
                    f"è¿›åº¦: {completed}/{total} ({completed/total*100:.1f}%) | "
                    f"å·²è€—æ—¶: {elapsed:.1f}ç§’ | "
                    f"é¢„è®¡å‰©ä½™: {remaining:.1f}ç§’ | "
                    f"ç´¯è®¡Tushareè®°å½•: {total_records} æ¡ | "
                    f"ç´¯è®¡å…¥åº“: æ’å…¥ {total_inserted} æ¡, æ›´æ–° {total_updated} æ¡"
                )

                # ä½¿ç”¨ä¸Kçº¿åŒæ­¥ç›¸åŒçš„ç»Ÿä¸€è¿›åº¦æ›´æ–°é€»è¾‘
                if task_id:
                    try:
                        task_name = "å¼€ç›˜ç«ä»·æ•°æ®"
                        unit_desc = "åªè‚¡ç¥¨" if force_sync else "ä¸ªäº¤æ˜“æ—¥"
                        current_item_name = f"æ­£åœ¨åŒæ­¥ç¬¬ {completed}/{total} {unit_desc}"
                        update_progress_with_consistent_logic(
                            task_id=task_id,
                            processed=completed,
                            total=total,
                            task_name=task_name,
                            current_item_name=current_item_name,
                        )
                    except Exception as e:
                        logger.warning(f"æ›´æ–°ç«ä»·æ•°æ®åŒæ­¥ä»»åŠ¡è¿›åº¦å¤±è´¥: {e}")

            def error_handler(item: str, e: Exception) -> Dict[str, Any]:
                if force_sync:
                    logger.warning(f"æŸ¥è¯¢æˆ–å¤„ç†è‚¡ç¥¨ {item} çš„ç«ä»·æ•°æ®å¤±è´¥: {e}ï¼Œè·³è¿‡")
                    return {
                        "code": item,
                        "count": 0,
                        "inserted": 0,
                        "updated": 0,
                        "error": True,
                    }
                else:
                    logger.warning(f"æŸ¥è¯¢æˆ–å¤„ç†äº¤æ˜“æ—¥ {item} çš„ç«ä»·æ•°æ®å¤±è´¥: {e}ï¼Œè·³è¿‡")
                    return {
                        "trade_date": item,
                        "codes": [],
                        "count": 0,
                        "inserted": 0,
                        "updated": 0,
                        "error": True,
                    }

            if force_sync:
                items = list(date_ranges.keys())
                worker_fn = worker_full
            else:
                items = trade_dates
                worker_fn = worker_incremental

            logger.info(
                f"å¼€å§‹å¹¶å‘åŒæ­¥å¼€ç›˜ç«ä»·æ•°æ® | "
                f"æ¨¡å¼: {'å…¨é‡(æŒ‰ä»£ç )' if force_sync else 'å¢é‡(æŒ‰äº¤æ˜“æ—¥)'} | "
                f"ä»»åŠ¡æ•°: {len(items)} | "
                f"å¹¶å‘æ•°: {max_workers}"
            )

            results = process_concurrently(
                items,
                worker_fn,
                max_workers=max_workers,
                error_handler=error_handler,
                progress_callback=progress_callback,
            )

            # èšåˆç»“æœï¼ˆä¸ K çº¿åŒæ­¥é£æ ¼ä¸€è‡´ï¼‰
            final_inserted = 0
            final_updated = 0
            error_count = 0
            synced_codes: List[str] = []

            for r in results:
                if not r:
                    continue
                inserted = int(r.get("inserted", 0) or 0)
                updated = int(r.get("updated", 0) or 0)
                is_error = bool(r.get("error", False))

                final_inserted += inserted
                final_updated += updated
                if is_error:
                    error_count += 1

                if force_sync:
                    code = r.get("code")
                    if code and (inserted > 0 or updated > 0):
                        synced_codes.append(code)
                else:
                    # å¢é‡æ¨¡å¼ï¼šworker æŒ‰äº¤æ˜“æ—¥è¿”å›æ¶‰åŠçš„è‚¡ç¥¨ä»£ç åˆ—è¡¨
                    r_codes = r.get("codes") or []
                    if inserted > 0 or updated > 0:
                        for c in r_codes:
                            if c and c not in synced_codes:
                                synced_codes.append(c)

            total_duration = time.time() - start_time

            logger.info(
                f"ğŸ å¹¶å‘åŒæ­¥å¼€ç›˜ç«ä»·æ•°æ®å®Œæˆ | "
                f"æ€»è€—æ—¶: {total_duration:.2f}ç§’ | "
                f"æˆåŠŸ: {len(synced_codes)} åªè‚¡ç¥¨ | "
                f"å¤±è´¥: {error_count} åªè‚¡ç¥¨ | "
                f"æ’å…¥: {final_inserted} | "
                f"æ›´æ–°: {final_updated}"
            )

            if not synced_codes:
                logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„å¼€ç›˜ç«ä»·æ•°æ®éœ€è¦æ›´æ–°")
                return {"inserted_count": 0, "updated_count": 0, "total": 0, "ts_codes": []}

            return {
                "inserted_count": final_inserted,
                "updated_count": final_updated,
                "total": final_inserted + final_updated,
                "ts_codes": synced_codes,  # è¿”å›å®é™…åŒæ­¥çš„è‚¡ç¥¨ä»£ç ï¼Œä¾›è°ƒç”¨æ–¹å¤„ç†ç¼“å­˜å¤±æ•ˆ
            }
            
        except CancellationException:
            total_duration = time.time() - start_time
            logger.info(f"å¼€ç›˜ç«ä»·åŒæ­¥å·²å–æ¶ˆ | è€—æ—¶: {total_duration:.2f}s")
            return {"inserted_count": 0, "updated_count": 0, "cancelled": True}
        except Exception as e:
            total_duration = time.time() - start_time
            logger.error(f"âŒ å¼€ç›˜ç«ä»·åŒæ­¥å¤±è´¥ | è€—æ—¶: {total_duration:.2f}s | é”™è¯¯: {e}")
            raise DatabaseException(f"åŒæ­¥å¼€ç›˜ç«ä»·æ•°æ®å¤±è´¥: {e}")

    # è·¯ç”±è–„åŒ–ï¼šç»Ÿä¸€ä»»åŠ¡åˆ›å»ºå…¥å£ï¼ˆä»…åšä¸šåŠ¡å±‚æ ¡éªŒä¸è°ƒåº¦è§¦å‘èšåˆï¼‰
    def create_kline_sync_tasks(
            self,
            selection: Dict[str, Any],
            periods: List[str],
            options: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        try:
            valid_periods = ("daily", "weekly", "monthly")
            if not periods or any(p not in valid_periods for p in periods):
                raise ValidationException(f"ä¸æ”¯æŒçš„å‘¨æœŸ: {periods}ï¼Œä»…æ”¯æŒ {valid_periods}")

            selection = selection or {}
            all_selected = bool(selection.get("all_selected", False))
            codes = selection.get("codes") or []
            if not all_selected and not codes:
                raise ValidationException("è¯·é€‰æ‹©è¦åŒæ­¥çš„è‚¡ç¥¨æˆ–ä½¿ç”¨å…¨é€‰")

            options = options or {}
            sync_kline = bool(options.get("sync_kline", True))  # é»˜è®¤åŒæ­¥Kçº¿
            sync_auction = bool(options.get("sync_auction", False))
            force_sync = bool(options.get("force_sync", False))
            # å¯é€‰çš„æ˜¾å¼æ—¥æœŸèŒƒå›´ï¼ˆé€šå¸¸æ¥è‡ªå‰ç«¯æ—¥å†ç­›é€‰ï¼‰
            start_date = options.get("start_date")
            end_date = options.get("end_date")

            from app.services import SchedulerService, scheduler_service

            # æ„å»ºä»»åŠ¡é€‰é¡¹
            task_options: Dict[str, Any] = {
                "force_sync": force_sync,
                "sync_kline": sync_kline,
            }
            if start_date and end_date:
                task_options["start_date"] = start_date
                task_options["end_date"] = end_date

            # å¦‚æœå‹¾é€‰äº†ç«ä»·æ•°æ®ï¼Œå°† sync_auction ç›¸å…³é€‰é¡¹ä¼ é€’ç»™ä»»åŠ¡
            if sync_auction and "daily" in periods:
                task_options["sync_auction"] = True
                task_options["ts_codes"] = codes if not all_selected else None
                task_options["all_selected"] = all_selected
            
            # åˆ›å»ºå•ä¸ªä»»åŠ¡ï¼Œå†…éƒ¨å¤„ç†æ‰€æœ‰å‘¨æœŸ
            req = SchedulerService.UnifiedKlineSyncRequest(
                subject_type=EntityTypes.STOCK,
                selection={"codes": codes, "all_selected": all_selected},
                periods=periods,  # ä¼ é€’å®Œæ•´periodsåˆ—è¡¨
                options=task_options,
            )
            result = scheduler_service.execute_kline_sync_unified(req)
            
            if not result.get("task_execution_id"):
                raise DatabaseException("æœªèƒ½åˆ›å»ºåŒæ­¥ä»»åŠ¡")

            # å¦‚æœä»»åŠ¡æœªåˆ›å»ºï¼ˆä¾‹å¦‚ï¼šåŒç±»å‹ä»»åŠ¡ running/cancellingï¼‰ï¼Œé€ä¼ ç»“æœ
            if result.get("success") is False:
                return {
                    "success": False,
                    "message": result.get("message", "ä»»åŠ¡æ­£åœ¨è¿è¡Œä¸­"),
                    "task_execution_id": result["task_execution_id"],
                }

            period_names = {"daily": "æ—¥çº¿", "weekly": "å‘¨çº¿", "monthly": "æœˆçº¿"}
            period_display = "ã€".join([period_names.get(p, p) for p in periods])
            auction_display = "ï¼ˆåŒ…å«ç«ä»·æ•°æ®ï¼‰" if sync_auction and "daily" in periods else ""

            return {
                "success": True,
                "message": f"è‚¡ç¥¨{period_display}åŒæ­¥ä»»åŠ¡å·²åˆ›å»º{auction_display}",
                "task_execution_id": result["task_execution_id"],
            }
        except (ValidationException, DatabaseException):
            raise
        except Exception as e:
            logger.error(f"åˆ›å»ºè‚¡ç¥¨åŒæ­¥ä»»åŠ¡å¤±è´¥: {e}")
            raise DatabaseException(str(e))


# åˆ›å»ºå…¨å±€æœåŠ¡å®ä¾‹
stock_kline_service = StockKlineService()
