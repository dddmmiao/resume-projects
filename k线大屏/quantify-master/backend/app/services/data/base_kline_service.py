"""
Kçº¿æœåŠ¡åŸºç±»
æä¾›é€šç”¨çš„Kçº¿æ•°æ®åŒæ­¥åŠŸèƒ½ï¼Œæ¶ˆé™¤ä»£ç é‡å¤
"""

from abc import ABC, abstractmethod
from typing import List, Dict, Any, Tuple, Optional
from datetime import datetime, timedelta, date
from loguru import logger

from app.core.exceptions import CancellationException, DatabaseException


class BaseKlineService(ABC):
    """Kçº¿æœåŠ¡åŸºç±»ï¼Œæä¾›é€šç”¨çš„Kçº¿æ•°æ®åŒæ­¥åŠŸèƒ½"""

    def __init__(self, entity_type: str):
        """
        åˆå§‹åŒ–åŸºç±»
        
        Args:
            entity_type: å®ä½“ç±»å‹ ('stock', 'bond', 'concept', 'industry')
        """
        self.entity_type = entity_type
        logger.debug(f"{entity_type} Kçº¿æœåŠ¡åˆå§‹åŒ–")

    def sync_kline_data_universal(
            self,
            ts_codes: List[str],
            periods=None,
            force_sync: bool = False,
            concurrent_workers: int = 0,
            task_id: str = None,
            start_date: Optional[str] = None,
            end_date: Optional[str] = None,
    ) -> Dict[str, int]:
        """
        é€šç”¨Kçº¿æ•°æ®åŒæ­¥æ–¹æ³•
        
        Args:
            ts_codes: ä»£ç åˆ—è¡¨
            periods: å‘¨æœŸç±»å‹åˆ—è¡¨
            force_sync: æ˜¯å¦å¼ºåˆ¶åŒæ­¥
            concurrent_workers: å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°
            task_id: ä»»åŠ¡IDï¼Œç”¨äºå–æ¶ˆæ£€æŸ¥
            start_date: å¼€å§‹æ—¥æœŸ (YYYYMMDDæ ¼å¼)
            end_date: ç»“æŸæ—¥æœŸ (YYYYMMDDæ ¼å¼)

        Returns:
            { "inserted_count": æ–°å¢æ¡æ•°, "updated_count": æ›´æ–°æ¡æ•° }
        """
        import time
        start_time = time.time()
        
        if periods is None:
            periods = ["daily"]
        
        try:
            # å¹¶å‘å¤„ç†ä»£ç åˆ—è¡¨ï¼ˆæ—¥æœŸèŒƒå›´åœ¨æ–¹æ³•å†…éƒ¨è·å–ï¼‰
            process_start_time = time.time()
            result, processed_codes = self._process_codes_concurrently(
                codes=ts_codes,
                periods=periods,
                force_sync=force_sync,
                concurrent_workers=concurrent_workers,
                task_id=task_id,
                start_date=start_date,
                end_date=end_date
            )
            process_duration = time.time() - process_start_time

            # åŒæ­¥å®Œæˆåçš„åç»­å¤„ç†ï¼šç¼“å­˜å¤±æ•ˆã€æ’åºå­—æ®µæ›´æ–°ã€æŠ€æœ¯æŒ‡æ ‡æ›´æ–°
            post_process_start_time = time.time()
            self._post_sync_processing(processed_codes, periods, force_sync, task_id)
            post_process_duration = time.time() - post_process_start_time

            total_duration = time.time() - start_time
            logger.info(
                f"{self.entity_type}Kçº¿åŒæ­¥å®Œæˆ | {result} | "
                f"æ€»è€—æ—¶: {total_duration:.2f}s | "
                f"æ•°æ®å¤„ç†: {process_duration:.2f}s | "
                f"åç»­å¤„ç†: {post_process_duration:.2f}s | "
                f"å¹³å‡æ¯ä»£ç : {total_duration/len(processed_codes):.3f}s" if processed_codes else f"æ€»è€—æ—¶: {total_duration:.2f}s"
            )
            return result

        except CancellationException:
            total_duration = time.time() - start_time
            logger.info(f"{self.entity_type}Kçº¿åŒæ­¥å·²å–æ¶ˆ | è€—æ—¶: {total_duration:.2f}s")
            return {"inserted_count": 0, "updated_count": 0, "cancelled": True}
        except Exception as e:
            total_duration = time.time() - start_time
            logger.error(f"âŒ {self.entity_type}Kçº¿åŒæ­¥å¤±è´¥ | è€—æ—¶: {total_duration:.2f}s | é”™è¯¯: {e}")
            raise DatabaseException(f"åŒæ­¥{self.entity_type}Kçº¿æ•°æ®å¤±è´¥: {e}")

    def _process_codes_concurrently(
            self,
            codes: List[str],
            periods: List[str],
            force_sync: bool,
            concurrent_workers: int,
            task_id: str,
            start_date: Optional[str] = None,
            end_date: Optional[str] = None
    ) -> Tuple[Dict[str, int], List[str]]:
        """
        å¹¶å‘å¤„ç†ä»£ç åˆ—è¡¨ï¼ˆæ—¥æœŸèŒƒå›´åœ¨æ–¹æ³•å†…éƒ¨è·å–ï¼‰
        
        Args:
            codes: ä»£ç åˆ—è¡¨
            periods: å‘¨æœŸåˆ—è¡¨
            force_sync: æ˜¯å¦å¼ºåˆ¶åŒæ­¥
            concurrent_workers: å¹¶å‘æ•°
            task_id: ä»»åŠ¡ID
            start_date: å¼€å§‹æ—¥æœŸ (YYYYMMDDæ ¼å¼)
            end_date: ç»“æŸæ—¥æœŸ (YYYYMMDDæ ¼å¼)
            
        Returns:
            (result_dict, processed_codes): ç»“æœå­—å…¸å’Œå·²å¤„ç†çš„ä»£ç åˆ—è¡¨
        """
        import time
        import threading
        from typing import Tuple as TypingTuple
        
        # æ‰¹é‡æ™ºèƒ½æ—¥æœŸèŒƒå›´è®¡ç®—ï¼šä¸ºæ¯ä¸ªä»£ç è®¡ç®—å„ä¸ªå‘¨æœŸçš„ç²¾ç¡®æ—¥æœŸèŒƒå›´
        from ..core.smart_date_range_calculator import SmartDateRangeCalculator

        # å¦‚æœæŒ‡å®šäº†æ—¥æœŸèŒƒå›´ï¼Œä½¿ç”¨æŒ‡å®šèŒƒå›´ï¼›å¦åˆ™ä½¿ç”¨æ™ºèƒ½è®¡ç®—
        if start_date and end_date:
            logger.info(f"ä½¿ç”¨æŒ‡å®šæ—¥æœŸèŒƒå›´ | {start_date} ~ {end_date}")
            # ä¸ºæ‰€æœ‰ä»£ç ä½¿ç”¨ç›¸åŒçš„æ—¥æœŸèŒƒå›´ï¼›ä¿æŒä¸ SmartDateRangeCalculator è¿”å›æ ¼å¼ä¸€è‡´
            # æ¯ä¸ªå‘¨æœŸå’Œ overall éƒ½ä½¿ç”¨ (start_date, end_date) å…ƒç»„ï¼Œä¾¿äºåç»­ç»Ÿä¸€è§£åŒ…
            period_ranges = {
                code: {
                    'overall': (start_date, end_date),
                    **{period: (start_date, end_date) for period in periods}
                }
                for code in codes
            }
        else:
            period_ranges = SmartDateRangeCalculator.calculate_period_ranges_for_codes(
                codes,
                periods,
                self.entity_type,
                force_sync
            )
        
        # æå–overallèŒƒå›´ï¼ˆç”¨äºfetchæ•°æ®ï¼‰
        date_ranges = {
            code: ranges['overall'] 
            for code, ranges in period_ranges.items() 
            if ranges.get('overall')
        }
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ•°æ®éƒ½æ˜¯æœ€æ–°çš„
        if not date_ranges:
            logger.info(
                f"æ‰€æœ‰{self.entity_type}Kçº¿æ•°æ®éƒ½æ˜¯æœ€æ–°çš„ï¼Œæ— éœ€åŒæ­¥ | "
                f"æ€»ä»£ç æ•°: {len(codes)}"
            )
            return {
                "inserted_count": 0,
                "updated_count": 0,
                "total": 0,
                "total_codes": len(codes),
                "periods": periods
            }, []
        
        logger.info(
            f"å¼€å§‹åŒæ­¥ {self.entity_type} Kçº¿æ•°æ® | "
            f"æ€»ä»£ç æ•°: {len(codes)}, "
            f"éœ€è¦åŒæ­¥: {len(date_ranges)}, "
            f"å‘¨æœŸ: {periods}, "
            f"force_sync={force_sync}, "
            f"å¹¶å‘æ•°: {concurrent_workers}"
        )
        
        # åˆå§‹åŒ–å‘¨æœŸå¤„ç†å™¨
        from app.services.data.kline_period_processor import KlinePeriodProcessor
        period_processor = KlinePeriodProcessor(self.entity_type)

        # å¯ç”¨å¹¶å‘æ‰§è¡Œ
        inserted_count = 0
        updated_count = 0
        processed_count = 0
        error_count = 0

        result_lock = threading.Lock()
        start_time = time.time()

        def worker(code_and_range: TypingTuple[str, TypingTuple[str, str]]) -> Dict[str, int]:
            code, (start_date, end_date) = code_and_range
            worker_start_time = time.time()
            local_inserted = 0
            local_updated = 0
            local_error = False

            try:
                # ä½¿ç”¨è¯¥ä»£ç çš„ç‰¹å®šæ—¥æœŸèŒƒå›´
                # é¦–å…ˆè·å–æ—¥çº¿æ•°æ®ï¼ˆä½œä¸ºåŸºç¡€æ•°æ®ï¼‰
                fetch_start_time = time.time()
                daily_kline_data = self._fetch_kline_from_tushare(
                    code, start_date, end_date, task_id=task_id
                )
                fetch_duration = time.time() - fetch_start_time

                if not daily_kline_data:
                    logger.warning(f"æœªè·å–åˆ°æ—¥çº¿æ•°æ® | ts_code: {code} | èŒƒå›´: {start_date}..{end_date} | è€—æ—¶: {fetch_duration:.3f}s")
                    return {"inserted": 0, "updated": 0}

                # ä½¿ç”¨å‘¨æœŸå¤„ç†å™¨å¤„ç†æ‰€æœ‰å‘¨æœŸ
                def period_progress_callback(period, completed, total):
                    """å‘¨æœŸå¤„ç†è¿›åº¦å›è°ƒ"""
                    logger.debug(f"{code} {period}å‘¨æœŸå¤„ç†å®Œæˆ ({completed}/{total})")
                
                # è·å–è¯¥ä»£ç çš„å‘¨æœŸèŒƒå›´
                code_period_ranges = period_ranges.get(code, {}) if period_ranges else {}
                
                result = period_processor.process_periods(
                    daily_data=daily_kline_data,
                    periods=periods,
                    bulk_store_func=lambda data, batch: self._bulk_store_data(data, batch),
                    batch_size=500,
                    progress_callback=period_progress_callback,
                    period_ranges=code_period_ranges
                )
                
                local_inserted = result.get("inserted_count", 0)
                local_updated = result.get("updated_count", 0)
                worker_duration = time.time() - worker_start_time

                logger.debug(
                    f"{code} åŒæ­¥å®Œæˆ | "
                    f"æ—¥æœŸ: {start_date}..{end_date} | "
                    f"è€—æ—¶: {worker_duration:.2f}s | "
                    f"æ’å…¥: {local_inserted} | æ›´æ–°: {local_updated}"
                )

            except Exception as e:
                worker_duration = time.time() - worker_start_time
                local_error = True
                logger.error(f"âŒ {code} åŒæ­¥å¤±è´¥ | è€—æ—¶: {worker_duration:.3f}s | é”™è¯¯: {e}")

            return {"inserted": local_inserted, "updated": local_updated, "error": local_error}

        max_workers = int(concurrent_workers or 1)
        if max_workers < 1:
            max_workers = 1

        # å°†å­—å…¸è½¬æ¢ä¸º (code, date_range) å…ƒç»„åˆ—è¡¨
        codes_with_ranges_list = list(date_ranges.items())
        
        logger.info(f"å¼€å§‹å¹¶å‘å¤„ç† {len(codes_with_ranges_list)} ä¸ªä»£ç ï¼Œå¹¶å‘æ•°: {max_workers}")

        # ç»Ÿä¸€å¹¶å‘æ‰§è¡Œ
        from app.utils.concurrent_utils import process_concurrently

        def progress_callback(result, completed, total):
            """è¿›åº¦å›è°ƒå‡½æ•°"""
            elapsed = time.time() - start_time
            avg_time = elapsed / completed if completed > 0 else 0
            remaining = (total - completed) * avg_time
            
            # æ›´æ–° Redis ä»»åŠ¡è¿›åº¦
            if task_id:
                try:
                    from app.services.scheduler.progress_utils import (
                        update_progress_with_consistent_logic,
                        get_task_type_chinese_mapping
                    )
                    
                    # è·å–ä»»åŠ¡ç±»å‹çš„ä¸­æ–‡æ˜ å°„
                    task_type_mapping = get_task_type_chinese_mapping("kline_sync")
                    task_type_chinese = task_type_mapping.get(self.entity_type, f"{self.entity_type}Kçº¿æ•°æ®")
                    
                    update_progress_with_consistent_logic(
                        task_id=task_id,
                        processed=completed,
                        total=total,
                        task_name=task_type_chinese,
                        current_item_name=f"å·²åŒæ­¥{completed}ä¸ª{task_type_chinese}"
                    )
                except Exception as e:
                    logger.warning(f"æ›´æ–°ä»»åŠ¡è¿›åº¦å¤±è´¥: {e}")
            
            logger.info(
                f"è¿›åº¦: {completed}/{total} ({completed/total*100:.1f}%) | "
                f"å·²è€—æ—¶: {elapsed:.1f}ç§’ | "
                f"é¢„è®¡å‰©ä½™: {remaining:.1f}ç§’"
            )

        results = process_concurrently(
            codes_with_ranges_list,
            worker,
            max_workers=max_workers,
            error_handler=lambda code_and_range, e: {"inserted": 0, "updated": 0},
            progress_callback=progress_callback
        )

        # èšåˆç»“æœ
        for r in results:
            with result_lock:
                inserted_count += r.get("inserted", 0)
                updated_count += r.get("updated", 0)
                if r.get("error", False):
                    error_count += 1
                processed_count += 1

        total_duration = time.time() - start_time
        logger.info(
            f"ğŸ å¹¶å‘å¤„ç†å®Œæˆ | "
            f"æ€»è€—æ—¶: {total_duration:.2f}ç§’ | "
            f"æˆåŠŸ: {processed_count - error_count} | "
            f"å¤±è´¥: {error_count} | "
            f"æ’å…¥: {inserted_count} | "
            f"æ›´æ–°: {updated_count}"
        )

        # è¿”å›ç»“æœå’Œå·²å¤„ç†çš„ä»£ç åˆ—è¡¨
        processed_codes = list(date_ranges.keys())
        return {
            "inserted_count": int(inserted_count),
            "updated_count": int(updated_count),
            "total": int(inserted_count) + int(updated_count),
            "total_codes": len(processed_codes),
            "periods": periods,
        }, processed_codes

    @staticmethod
    def _parse_yyyymmdd(date_str: str) -> date:
        """è§£æ YYYYMMDD æ ¼å¼çš„æ—¥æœŸå­—ç¬¦ä¸²ä¸º date å¯¹è±¡"""
        return datetime.strptime(date_str, "%Y%m%d").date()

    @staticmethod
    def is_valid_weekly_range(start_date: str, end_date: str) -> bool:
        """æ ¡éªŒç”¨äºå‘¨çº¿åŒæ­¥çš„æ—¥æœŸèŒƒå›´æ˜¯å¦ç¬¦åˆä¸šåŠ¡è§„åˆ™ã€‚

        è§„åˆ™ï¼š
        - å¦‚æœç»“æŸæ—¥æœŸåœ¨å½“å‰å‘¨ä¹‹å‰ï¼ˆå®Œå…¨å±äºè¿‡å»å‘¨ï¼‰ï¼š
          - èµ·å§‹æ—¥æœŸå¿…é¡»æ˜¯è¯¥å‘¨çš„å‘¨ä¸€
          - ç»“æŸæ—¥æœŸå¿…é¡»æ˜¯æŸå‘¨çš„å‘¨æ—¥
          - æ€»å¤©æ•°å¿…é¡»æ˜¯ 7 çš„æ•´æ•°å€ï¼ˆè‹¥å¹²å®Œæ•´è‡ªç„¶å‘¨ï¼‰
        - å¦‚æœèŒƒå›´è¦†ç›–å½“å‰å‘¨ï¼š
          - å½“å‰å‘¨å…è®¸ä¸å®Œæ•´
          - ä½†å¦‚æœèµ·å§‹æ—¥æœŸæ—©äºå½“å‰å‘¨çš„å‘¨ä¸€ï¼Œåˆ™èµ·å§‹æ—¥æœŸå¿…é¡»æ˜¯å…¶æ‰€åœ¨å‘¨çš„å‘¨ä¸€ï¼Œé¿å…æˆªæ–­è¿‡å»å‘¨
        """
        try:
            start = BaseKlineService._parse_yyyymmdd(start_date)
            end = BaseKlineService._parse_yyyymmdd(end_date)
        except Exception:
            return False

        if start > end:
            return False

        today = datetime.today().date()
        cur_week_start = today - timedelta(days=today.weekday())

        def week_start(d: date) -> date:
            return d - timedelta(days=d.weekday())

        def week_end(d: date) -> date:
            return week_start(d) + timedelta(days=6)

        # å®Œå…¨åœ¨è¿‡å»å‘¨ï¼šå¿…é¡»æ˜¯è‹¥å¹²å®Œæ•´è‡ªç„¶å‘¨
        if end < cur_week_start:
            if start != week_start(start):
                return False
            if end != week_end(end):
                return False
            delta_days = (end - start).days + 1
            return delta_days % 7 == 0

        # è¦†ç›–å½“å‰å‘¨ï¼šå½“å‰å‘¨å…è®¸ä¸å®Œæ•´ï¼Œä½†ä¸èƒ½æˆªæ–­æ›´æ—©çš„å‘¨
        if start < cur_week_start and start != week_start(start):
            return False

        return True

    @staticmethod
    def validate_period_date_range(period: str, start_date: str, end_date: str, entity_name: str = "") -> Tuple[bool, str]:
        """ç»Ÿä¸€æ ¡éªŒæŒ‡å®šå‘¨æœŸçš„æ—¥æœŸèŒƒå›´æ˜¯å¦åˆæ³•
        
        Args:
            period: å‘¨æœŸç±»å‹ (daily/weekly/monthly)
            start_date: å¼€å§‹æ—¥æœŸ (YYYYMMDD)
            end_date: ç»“æŸæ—¥æœŸ (YYYYMMDD)
            entity_name: å®ä½“åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰ï¼Œå¦‚"è‚¡ç¥¨"ã€"å¯è½¬å€º"ç­‰
        
        Returns:
            (is_valid, warning_message): æ˜¯å¦åˆæ³•å’Œè­¦å‘Šä¿¡æ¯
        """
        if period == "daily":
            return True, ""  # æ—¥çº¿ä¸éœ€è¦æ ¡éªŒ
        
        period_name_map = {"weekly": "å‘¨çº¿", "monthly": "æœˆçº¿"}
        period_name = period_name_map.get(period, period)
        
        if period == "weekly":
            is_valid = BaseKlineService.is_valid_weekly_range(start_date, end_date)
            if not is_valid:
                msg = f"è·³è¿‡{entity_name}{period_name}åŒæ­¥ï¼šæ—¥æœŸèŒƒå›´ {start_date}~{end_date} ä¸æ»¡è¶³å®Œæ•´å‘¨è§„åˆ™"
                return False, msg
        elif period == "monthly":
            is_valid = BaseKlineService.is_valid_monthly_range(start_date, end_date)
            if not is_valid:
                msg = f"è·³è¿‡{entity_name}{period_name}åŒæ­¥ï¼šæ—¥æœŸèŒƒå›´ {start_date}~{end_date} ä¸æ»¡è¶³å®Œæ•´æœˆè§„åˆ™"
                return False, msg
        
        return True, ""

    @staticmethod
    def is_valid_monthly_range(start_date: str, end_date: str) -> bool:
        """æ ¡éªŒç”¨äºæœˆçº¿åŒæ­¥çš„æ—¥æœŸèŒƒå›´æ˜¯å¦ç¬¦åˆä¸šåŠ¡è§„åˆ™ã€‚

        è§„åˆ™ï¼š
        - å¦‚æœç»“æŸæ—¥æœŸåœ¨å½“å‰æœˆä¹‹å‰ï¼ˆå®Œå…¨å±äºè¿‡å»æœˆä»½ï¼‰ï¼š
          - èµ·å§‹æ—¥æœŸå¿…é¡»æ˜¯æ‰€åœ¨æœˆä»½çš„ 1 å·
          - ç»“æŸæ—¥æœŸå¿…é¡»æ˜¯æ‰€åœ¨æœˆä»½çš„æœ€åä¸€å¤©
        - å¦‚æœèŒƒå›´è¦†ç›–å½“å‰æœˆï¼š
          - å½“å‰æœˆå…è®¸ä¸å®Œæ•´
          - ä½†å¦‚æœèµ·å§‹æ—¥æœŸæ—©äºå½“æœˆ 1 å·ï¼Œåˆ™èµ·å§‹æ—¥æœŸå¿…é¡»æ˜¯å…¶æ‰€åœ¨æœˆä»½çš„ 1 å·ï¼Œé¿å…æˆªæ–­è¿‡å»æœˆä»½
        """
        try:
            start = BaseKlineService._parse_yyyymmdd(start_date)
            end = BaseKlineService._parse_yyyymmdd(end_date)
        except Exception:
            return False

        if start > end:
            return False

        today = datetime.today().date()
        cur_month_start = today.replace(day=1)

        def month_start(d: date) -> date:
            return d.replace(day=1)

        def month_end(d: date) -> date:
            # åˆ©ç”¨â€œä¸‹æœˆ 1 å·å‡ 1 å¤©â€å¾—åˆ°å½“æœˆæœ€åä¸€å¤©
            next_month = (d.replace(day=28) + timedelta(days=4)).replace(day=1)
            return next_month - timedelta(days=1)

        # å®Œå…¨åœ¨è¿‡å»æœˆä»½ï¼šå¿…é¡»æ˜¯è‹¥å¹²å®Œæ•´è‡ªç„¶æœˆ
        if end < cur_month_start:
            if start != month_start(start):
                return False
            if end != month_end(end):
                return False
            return True

        # è¦†ç›–å½“å‰æœˆï¼šå½“å‰æœˆå…è®¸ä¸å®Œæ•´ï¼Œä½†ä¸èƒ½æˆªæ–­æ›´æ—©çš„æœˆä»½
        if start < cur_month_start and start != month_start(start):
            return False

        return True

    @abstractmethod
    def _fetch_kline_from_tushare(
            self, ts_code: str, start_date: str, end_date: str, task_id: str = None
    ) -> List[Dict[str, Any]]:
        """
        ä»Tushareè·å–Kçº¿æ•°æ®ï¼ˆå­ç±»å®ç°ï¼‰
        
        Args:
            ts_code: ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            task_id: ä»»åŠ¡ID
            
        Returns:
            Kçº¿æ•°æ®åˆ—è¡¨
        """
        pass

    @abstractmethod
    def _bulk_store_data(
            self,
            data: List[Dict[str, Any]],
            batch_size: int = 500
    ) -> Dict[str, int]:
        """
        æ‰¹é‡å­˜å‚¨Kçº¿æ•°æ®ï¼ˆå­ç±»å®ç°ï¼‰
        
        Args:
            data: Kçº¿æ•°æ®åˆ—è¡¨ï¼ˆå·²é€šè¿‡ä¸Šå±‚è¿‡æ»¤ï¼‰
            batch_size: æ‰¹é‡å¤§å°
            
        Returns:
            å­˜å‚¨ç»“æœ
        """
        pass

    def _post_sync_processing(self, codes: List[str], periods: List[str], force_sync: bool, task_id: str = None):
        """
        åŒæ­¥å®Œæˆåçš„åç»­å¤„ç†
        
        Args:
            codes: ä»£ç åˆ—è¡¨
            periods: å‘¨æœŸåˆ—è¡¨
            force_sync: æ˜¯å¦å¼ºåˆ¶åŒæ­¥
            task_id: ä»»åŠ¡IDï¼Œç”¨äºæ£€æŸ¥å–æ¶ˆçŠ¶æ€
        """
        try:
            from app.services.management import indicator_updater

            # å¤±æ•ˆKçº¿æœ€æ–°æ—¥æœŸç¼“å­˜
            try:
                from app.services.core.cache_service import cache_service
                from app.constants.table_types import TableTypes
                table_type = TableTypes.entity_type_to_table_type(self.entity_type)
                deleted_count = cache_service.invalidate_kline_latest_dates(table_type)
                logger.debug(f"å¤±æ•ˆ{table_type}Kçº¿æœ€æ–°æ—¥æœŸç¼“å­˜ | åˆ é™¤: {deleted_count}")
            except Exception as e:
                logger.warning(f"å¤±æ•ˆKçº¿æœ€æ–°æ—¥æœŸç¼“å­˜å¤±è´¥: {e}")

            # åœ¨å¼€å§‹æŒ‡æ ‡è®¡ç®—å‰å†æ¬¡æ£€æŸ¥å–æ¶ˆçŠ¶æ€
            if task_id:
                from app.services.core.redis_task_manager import redis_task_manager
                if redis_task_manager.is_task_cancelled(task_id):
                    logger.info(f"ä»»åŠ¡åœ¨æŒ‡æ ‡è®¡ç®—å‰è¢«å–æ¶ˆ | task_id: {task_id}")
                    raise CancellationException("ä»»åŠ¡å·²å–æ¶ˆ")

            # ç¬¬äºŒæ­¥ï¼šå¼‚æ­¥æ›´æ–°æ•°æ®
            for period in periods:
                # å¼‚æ­¥æ›´æ–°æŠ€æœ¯æŒ‡æ ‡
                try:
                    def on_indicator_complete(e_type: str, p: str, e_codes: List[str], success: bool, updated_rows: int):
                        """æŒ‡æ ‡æ›´æ–°å®Œæˆåçš„å›è°ƒï¼šå¤±æ•ˆç¼“å­˜"""
                        try:
                            # å¤±æ•ˆKçº¿ç¼“å­˜ï¼ˆç¼“å­˜é¢„çƒ­å·²å…³é—­ï¼Œæ”¹ä¸ºæ‡’åŠ è½½æ¨¡å¼ï¼‰
                            self._invalidate_cache(p, e_codes)
                            logger.debug(f"å¤±æ•ˆ{e_type} {p}Kçº¿ç¼“å­˜å®Œæˆ")
                        except Exception as e:
                            logger.warning(f"{e_type}{p}ç¼“å­˜å¤„ç†å¤±è´¥: {e}")

                    indicator_updater.async_sync_indicators(
                        entity_type=self.entity_type, 
                        entity_codes=codes, 
                        period=period, 
                        force_sync=force_sync,
                        on_complete=on_indicator_complete  # æŒ‡æ ‡æ›´æ–°å®Œæˆåæ‰§è¡Œç¼“å­˜å¤±æ•ˆ
                    )
                except Exception as e:
                    logger.error(f"å¯åŠ¨{self.entity_type}{period}æŠ€æœ¯æŒ‡æ ‡å¼‚æ­¥æ›´æ–°å¤±è´¥: {e}")
                    # å¦‚æœæŠ€æœ¯æŒ‡æ ‡æ›´æ–°å¯åŠ¨å¤±è´¥ï¼Œä»ç„¶å¤±æ•ˆç¼“å­˜ï¼ˆé¿å…ç¼“å­˜æ•°æ®ä¸æ›´æ–°ï¼‰
                    try:
                        self._invalidate_cache(period, codes)
                        logger.warning(f"{self.entity_type}{period}æŠ€æœ¯æŒ‡æ ‡æ›´æ–°å¯åŠ¨å¤±è´¥ï¼Œå·²å¤±æ•ˆç¼“å­˜ä»¥ç¡®ä¿æ•°æ®ä¸€è‡´æ€§")
                    except Exception as cache_e:
                        logger.warning(f"{self.entity_type}{period}ç¼“å­˜å¤±æ•ˆå¤±è´¥: {cache_e}")
        except Exception as e:
            logger.error(f"{self.entity_type} Kçº¿åŒæ­¥åç»­å¤„ç†å¤±è´¥: {e}")

    @abstractmethod
    def _invalidate_cache(self, period: str, codes: List[str]):
        """
        å¤±æ•ˆç¼“å­˜ï¼ˆå­ç±»å®ç°ï¼‰
        
        Args:
            period: å‘¨æœŸ
            codes: ä»£ç åˆ—è¡¨
        """
        pass

    def _preheat_cache(self, period: str, codes: List[str]):
        """
        é¢„çƒ­ç¼“å­˜ï¼šæŒ‡æ ‡è®¡ç®—å®Œæˆåï¼Œé‡æ–°åŠ è½½æ•°æ®åˆ°ç¼“å­˜
        
        ä¼˜åŒ–ç­–ç•¥ï¼š
        1. åˆ†æ‰¹å¤„ç†ï¼Œé¿å…å†…å­˜å³°å€¼è¿‡é«˜
        2. é™ä½å¹¶å‘æ•°ï¼Œå‡å°‘æ•°æ®åº“/Rediså‹åŠ›
        3. æ‰¹æ¬¡é—´æ·»åŠ é—´éš”ï¼Œè®©ç³»ç»Ÿæœ‰å–˜æ¯æ—¶é—´
        
        Args:
            period: å‘¨æœŸ
            codes: ä»£ç åˆ—è¡¨
        """
        def warmup_task():
            import time
            from concurrent.futures import ThreadPoolExecutor, as_completed
            from app.utils.concurrent_utils import ConcurrentConfig
            
            start_time = time.time()
            total_codes = len(codes)
            
            try:
                # è·å–ç¼“å­˜æ–¹æ³•ï¼ˆå­ç±»å®ç°ï¼‰
                get_full_fn = self._get_kline_data_full_method(period)
                if not get_full_fn:
                    logger.warning(f"{self.entity_type} æœªå®ç°ç¼“å­˜é¢„çƒ­æ–¹æ³•")
                    return
                
                # ä¼˜åŒ–å‚æ•°ï¼šä½¿ç”¨è‡ªé€‚åº”å¹¶å‘æ•°ï¼Œåˆ†æ‰¹å¤„ç†
                max_workers = max(1, ConcurrentConfig.get_optimal_workers() // 2)  # ä½¿ç”¨ä¸€åŠçš„å¹¶å‘æ•°ï¼Œå‡å°‘èµ„æºå‹åŠ›
                batch_size = 50  # æ¯æ‰¹å¤„ç†50ä¸ªä»£ç 
                batch_interval = 0.3  # æ‰¹æ¬¡é—´éš”0.3ç§’
                
                success_count = 0
                completed_count = 0
                log_interval = max(1, total_codes // 10)
                
                # åˆ†æ‰¹å¤„ç†
                for batch_start in range(0, total_codes, batch_size):
                    batch_end = min(batch_start + batch_size, total_codes)
                    batch_codes = codes[batch_start:batch_end]
                    
                    with ThreadPoolExecutor(max_workers=max_workers) as executor:
                        futures = {executor.submit(get_full_fn, code): code for code in batch_codes}
                        
                        for future in as_completed(futures):
                            completed_count += 1
                            try:
                                future.result()
                                success_count += 1
                            except Exception:
                                pass
                            
                            # è¾“å‡ºè¿›åº¦
                            if completed_count % log_interval == 0 or completed_count == total_codes:
                                elapsed = time.time() - start_time
                                progress_pct = completed_count / total_codes * 100
                                avg_time = elapsed / completed_count
                                remaining_time = avg_time * (total_codes - completed_count)
                                logger.info(
                                    f"{self.entity_type} {period} ç¼“å­˜é¢„çƒ­è¿›åº¦: {completed_count}/{total_codes} ({progress_pct:.1f}%) | "
                                    f"å·²è€—æ—¶: {elapsed:.1f}s | é¢„è®¡å‰©ä½™: {remaining_time:.1f}s"
                                )
                    
                    # æ‰¹æ¬¡é—´ä¼‘æ¯ï¼Œè®©ç³»ç»Ÿæœ‰å–˜æ¯æ—¶é—´
                    if batch_end < total_codes:
                        time.sleep(batch_interval)
                
                elapsed = time.time() - start_time
                logger.info(f"{self.entity_type} {period}ç¼“å­˜é¢„çƒ­å®Œæˆ: {success_count}/{total_codes} | æ€»è€—æ—¶: {elapsed:.2f}s")
            except Exception as e:
                elapsed = time.time() - start_time
                logger.warning(f"{self.entity_type} {period}ç¼“å­˜é¢„çƒ­å¤±è´¥: {e} | è€—æ—¶: {elapsed:.2f}s")
        
        # ä½¿ç”¨é€šç”¨å¼‚æ­¥å·¥å…·æ‰§è¡Œé¢„çƒ­ï¼Œä¸é˜»å¡ä¸»æµç¨‹
        from app.utils.concurrent_utils import run_async
        run_async(warmup_task, name=f"cache_warmup_{self.entity_type}_{period}")

    def _get_kline_data_full_method(self, period: str):
        """
        è·å–å¸¦ç¼“å­˜çš„Kçº¿æ•°æ®æ–¹æ³•ï¼ˆå­ç±»å¯è¦†ç›–ï¼‰
        
        Args:
            period: å‘¨æœŸ
            
        Returns:
            å¯è°ƒç”¨çš„ç¼“å­˜æ–¹æ³•ï¼Œæˆ–None
        """
        return None
