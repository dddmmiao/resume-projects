"""
technical_indicator_updater: è®¡ç®—å¹¶å›å¡«æŠ€æœ¯æŒ‡æ ‡åˆ°åˆ†å¹´Kçº¿è¡¨
- å…¨é‡ï¼šä¸ºç»™å®š entity_type ä¸ä»£ç é›†åˆï¼Œé‡ç®— daily/weekly/monthly æŒ‡æ ‡
- å¢é‡ï¼šä¸ºç»™å®š entity_type/period/ä»£ç é›†åˆï¼Œä»…å¯¹æœ€è¿‘ä¸€æ®µç¼ºå£ä¸æ–°æ•°æ®è¿›è¡Œæ›´æ–°

è¯´æ˜ï¼š
- å…ˆå®ç° EXPMAï¼ˆ5/10/20/60ï¼‰ä¸ºç¤ºä¾‹ï¼Œå…¶ä½™æŒ‡æ ‡ç•™æœ‰æ‰©å±•ä½ï¼ˆæ¥å£ä¸å˜ï¼Œåç»­è¡¥é½ï¼‰
- ä½¿ç”¨ TableFactory ä¸ KlineQueryUtils è·å–æ•°æ®ï¼›ç”¨ SQLAlchemy session æ‰¹é‡ upsert
"""
from __future__ import annotations

import math
import os
from typing import List, Dict, Any, Optional, Callable

from loguru import logger
from sqlalchemy import text
from sqlalchemy.orm import Session

from app.models import get_db, TableFactory
from ..data.concept_kline_service import ConceptKlineService
from ..data.convertible_bond_kline_service import ConvertibleBondKlineService
from ..data.indicator_service import indicator_service
from ..data.industry_kline_service import IndustryKlineService
# ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šå¯¼å…¥ä¸šåŠ¡å±‚Kçº¿æœåŠ¡ï¼Œä½¿ç”¨å¸¦ç¼“å­˜çš„æ–¹æ³•
from ..data.stock_kline_service import StockKlineService
from ...constants.table_types import TableTypes


class TechnicalIndicatorUpdater:
    def __init__(self) -> None:
        self.expma_periods = [5, 10, 20, 60, 250]
        self.ma_periods = [5, 10, 20, 60, 250]
        # æŒ‰éœ€è®¡ç®—æŒ‡æ ‡é…ç½®ï¼šåªè®¡ç®—ç­–ç•¥å®é™…ä½¿ç”¨çš„æŒ‡æ ‡
        # å½“å‰ç­–ç•¥åªä½¿ç”¨EXPMAï¼Œå…¶ä»–æŒ‡æ ‡æš‚ä¸è®¡ç®—
        # å¦‚æœæœªæ¥ç­–ç•¥éœ€è¦ä½¿ç”¨å…¶ä»–æŒ‡æ ‡ï¼Œå¯ä»¥åœ¨è¿™é‡Œæ·»åŠ 
        self.required_indicators = {
            'expma': True,  # EXPMAç­–ç•¥ä½¿ç”¨
            'ma': False,    # æš‚ä¸ä½¿ç”¨ï¼Œä½†ä¿ç•™ï¼ˆBOLLéœ€è¦MA20ï¼‰
            'macd': False,  # æš‚ä¸ä½¿ç”¨
            'rsi': False,  # æš‚ä¸ä½¿ç”¨
            'kdj': False,  # æš‚ä¸ä½¿ç”¨
            'boll': False,  # æš‚ä¸ä½¿ç”¨
            'cci': False,  # æš‚ä¸ä½¿ç”¨
            'wr': False,  # æš‚ä¸ä½¿ç”¨
            'dmi': False,  # æš‚ä¸ä½¿ç”¨
            'sar': False,  # æš‚ä¸ä½¿ç”¨
            'obv': False,  # æš‚ä¸ä½¿ç”¨
            'td': False,  # æš‚ä¸ä½¿ç”¨
        }
        
        # ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šåˆå§‹åŒ–ä¸šåŠ¡å±‚Kçº¿æœåŠ¡å®ä¾‹ï¼Œä½¿ç”¨å¸¦ç¼“å­˜çš„æ–¹æ³•
        self.stock_kline_service = StockKlineService()
        self.concept_kline_service = ConceptKlineService()
        self.industry_kline_service = IndustryKlineService()
        self.bond_kline_service = ConvertibleBondKlineService()
        
    def _get_kline_data_for_indicator(self, ts_code: str, period: str, table_type: str) -> List[Dict[str, Any]]:
        import time
        start_time = time.time()

        if table_type == TableTypes.STOCK:
            data = self.stock_kline_service._get_stock_kline_data_full(ts_code, period, use_cache=False)
            logger.debug(f"æŒ‡æ ‡è®¡ç®—è·å–è‚¡ç¥¨Kçº¿ | ts_code: {ts_code} | period: {period} | æ•°æ®é‡: {len(data)} | è€—æ—¶: {time.time() - start_time:.3f}s")
            return data
        elif table_type == TableTypes.CONCEPT:
            data = self.concept_kline_service._get_concept_kline_data_full(ts_code, period, use_cache=False)
            logger.debug(f"æŒ‡æ ‡è®¡ç®—è·å–æ¦‚å¿µKçº¿ | ts_code: {ts_code} | period: {period} | æ•°æ®é‡: {len(data)} | è€—æ—¶: {time.time() - start_time:.3f}s")
            return data
        elif table_type == TableTypes.INDUSTRY:
            data = self.industry_kline_service._get_industry_kline_data_full(ts_code, period, use_cache=False)
            logger.debug(f"æŒ‡æ ‡è®¡ç®—è·å–è¡Œä¸šKçº¿ | ts_code: {ts_code} | period: {period} | æ•°æ®é‡: {len(data)} | è€—æ—¶: {time.time() - start_time:.3f}s")
            return data
        elif table_type == TableTypes.CONVERTIBLE_BOND:
            data = self.bond_kline_service._get_convertible_bond_kline_data_full(ts_code, period, use_cache=False)
            logger.debug(f"æŒ‡æ ‡è®¡ç®—è·å–å¯è½¬å€ºKçº¿ | ts_code: {ts_code} | period: {period} | æ•°æ®é‡: {len(data)} | è€—æ—¶: {time.time() - start_time:.3f}s")
            return data
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„table_type: {table_type}ï¼Œè¯·æ£€æŸ¥é…ç½®æˆ–æ·»åŠ æ”¯æŒ")

    @staticmethod
    def _norm_list(seq: List[Any], n: int) -> List[Any]:
        m = len(seq)
        if m == n:
            return list(seq)
        if m > n:
            return list(seq)[-n:]
        # m < n, å·¦ä¾§è¡¥é½
        return [None] * (n - m) + list(seq)

    @staticmethod
    def _norm_map(series_map: Dict[Any, List[Any]], n: int) -> Dict[Any, List[Any]]:
        return {k: TechnicalIndicatorUpdater._norm_list(v, n) for k, v in (series_map or {}).items()}

    @staticmethod
    def _sanitize_val(v: Any) -> Any:
        try:
            if isinstance(v, float) and (math.isnan(v) or math.isinf(v)):
                return None
        except Exception:
            pass
        return v

    def async_sync_indicators(self, *, entity_type: str, entity_codes: List[str], period: Optional[str] = None,
                              force_sync: bool = False,
                              on_complete: Optional[Callable[[str, str, List[str], bool, int], None]] = None) -> None:
        """
        å¼‚æ­¥æ›´æ–°æŠ€æœ¯æŒ‡æ ‡ï¼ˆå†…éƒ¨è°ƒç”¨ sync_indicatorsï¼‰
        
        Args:
            entity_type: å®ä½“ç±»å‹
            entity_codes: å®ä½“ä»£ç åˆ—è¡¨
            period: å‘¨æœŸï¼ˆdaily/weekly/monthlyï¼‰
            force_sync: æ˜¯å¦å¼ºåˆ¶åŒæ­¥
            on_complete: å®Œæˆå›è°ƒå‡½æ•°ï¼Œæ¥æ”¶ (entity_type, period, entity_codes, success, updated_rows) å‚æ•°
        """
        if not entity_codes:
            return
        # è§„èŒƒå‘¨æœŸ
        if period not in ("daily", "weekly", "monthly"):
            period = "daily"

        def _job():
            # è°ƒç”¨åŒæ­¥æ–¹æ³•
            result = self.sync_indicators(
                entity_type=entity_type,
                entity_codes=entity_codes,
                period=period,
                force_sync=force_sync,
            )
            success = result.get("success", False)
            updated = result.get("updated_count", 0)
            
            # è°ƒç”¨å®Œæˆå›è°ƒï¼ˆå¦‚æœæä¾›ï¼‰
            if on_complete:
                try:
                    on_complete(entity_type, period, entity_codes, success, updated)
                except Exception as e:
                    logger.warning(f"æ‰§è¡ŒæŒ‡æ ‡æ›´æ–°å®Œæˆå›è°ƒå¤±è´¥: {e}")

        from app.utils.concurrent_utils import run_async
        run_async(_job, name=f"indicator_{entity_type}_{period}")

    def sync_indicators(self, *, entity_type: str, entity_codes: List[str], period: Optional[str] = None,
                        force_sync: bool = False) -> Dict[str, Any]:
        """
        åŒæ­¥æ›´æ–°æŠ€æœ¯æŒ‡æ ‡ï¼ˆé˜»å¡æ‰§è¡Œï¼‰
        
        Args:
            entity_type: å®ä½“ç±»å‹
            entity_codes: å®ä½“ä»£ç åˆ—è¡¨
            period: å‘¨æœŸï¼ˆdaily/weekly/monthlyï¼‰
            force_sync: æ˜¯å¦å¼ºåˆ¶åŒæ­¥
            
        Returns:
            {"success": bool, "updated_count": int}
        """
        if not entity_codes:
            return {"success": True, "updated_count": 0}
        
        # è§„èŒƒå‘¨æœŸ
        if period not in ("daily", "weekly", "monthly"):
            period = "daily"
        
        try:
            logger.info(f"å¼€å§‹åŒæ­¥æŒ‡æ ‡æ›´æ–°: {entity_type}, {period}, codes={len(entity_codes)}")
            updated = self._update_indicators_for_period(
                entity_type=entity_type,
                entity_codes=entity_codes,
                period=period,
                force_sync=force_sync,
            )
            logger.info(f"åŒæ­¥æŒ‡æ ‡æ›´æ–°å®Œæˆ: {entity_type}, {period}, æ›´æ–°è¡Œæ•°={updated}")
            return {"success": True, "updated_count": updated}
        except Exception as e:
            logger.error(f"åŒæ­¥æŒ‡æ ‡æ›´æ–°å¤±è´¥: {entity_type}, {period}: {e}")
            return {"success": False, "updated_count": 0, "error": str(e)}

    def _build_indicator_updates_for_code(
        self,
        code: str,
        period: str,
        table_type: str,
        force_sync: bool,
    ) -> List[Dict[str, Any]]:
        """ä¸ºå•ä¸ªä»£ç æ„å»ºéœ€è¦å†™å…¥çš„æŒ‡æ ‡æ›´æ–°åˆ—è¡¨ï¼ˆå¢é‡å°¾éƒ¨æ›´æ–°ï¼‰ã€‚"""
        kline_dicts: List[Dict[str, Any]] = self._get_kline_data_for_indicator(
            ts_code=code,
            period=period,
            table_type=table_type,
        )
        if not kline_dicts:
            return []

        kline_sorted = kline_dicts
        n = len(kline_sorted)

        # æŠ½å–åŸºç¡€åºåˆ—
        try:
            close = [float(d.get("close")) for d in kline_sorted if d.get("close") is not None]
            high = [float(d.get("high")) for d in kline_sorted if d.get("high") is not None]
            low = [float(d.get("low")) for d in kline_sorted if d.get("low") is not None]
            vol = [float(d.get("vol")) if d.get("vol") is not None else 0.0 for d in kline_sorted]
        except Exception:
            return []

        if len(close) != n or len(high) != n or len(low) != n:
            return []

        # è®¡ç®—æŒ‡æ ‡ï¼ˆå…ˆæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼Œé¿å…é‡å¤è®¡ç®—ï¼›æŒ‰éœ€è®¡ç®—ï¼Œåªè®¡ç®—é…ç½®ä¸­éœ€è¦çš„æŒ‡æ ‡ï¼‰
        try:
            # æŒ‰ required_indicators æ§åˆ¶æ˜¯å¦è®¡ç®—å„ä¸ªæŒ‡æ ‡
            skip_expma = not self.required_indicators.get('expma', False)
            skip_ma = not self.required_indicators.get('ma', False)
            skip_macd = not self.required_indicators.get('macd', False)
            skip_rsi = not self.required_indicators.get('rsi', False)
            skip_kdj = not self.required_indicators.get('kdj', False)
            skip_boll = not self.required_indicators.get('boll', False)
            skip_cci = not self.required_indicators.get('cci', False)
            skip_wr = not self.required_indicators.get('wr', False)
            skip_dmi = not self.required_indicators.get('dmi', False)
            skip_sar = not self.required_indicators.get('sar', False)
            skip_obv = not self.required_indicators.get('obv', False)
            skip_td = not self.required_indicators.get('td', False)

            # åªè®¡ç®—éœ€è¦çš„æŒ‡æ ‡
            if skip_expma:
                expma_map = {}
            else:
                expma_map = indicator_service.compute_expma_series(close, self.expma_periods)

            if skip_ma:
                ma_map = {}
            else:
                ma_map = indicator_service.compute_ma_series(close, self.ma_periods)

            if skip_macd:
                macd = {}
            else:
                # MACDå¤ç”¨å·²è®¡ç®—çš„EXPMA12å’ŒEXPMA26
                expma_12 = expma_map.get(12) if not skip_expma and 12 in expma_map else None
                expma_26 = expma_map.get(26) if not skip_expma and 26 in expma_map else None
                macd = indicator_service.compute_macd(
                    close,
                    expma_fast=expma_12,
                    expma_slow=expma_26
                )

            if skip_rsi:
                rsi_map = {}
            else:
                rsi_map = indicator_service.compute_rsi(close, [6, 12, 24])

            if skip_kdj:
                kdj = {}
            else:
                kdj = indicator_service.compute_kdj(high, low, close)

            if skip_boll:
                boll = {}
            else:
                # BOLLå¤ç”¨å·²è®¡ç®—çš„MA20
                ma_20 = ma_map.get(20) if not skip_ma and 20 in ma_map else None
                boll = indicator_service.compute_boll(close, ma20=ma_20)

            if skip_cci:
                cci = []
            else:
                cci = indicator_service.compute_cci(high, low, close)

            if skip_wr:
                wr = []
            else:
                wr = indicator_service.compute_wr(high, low, close)

            if skip_dmi:
                dmi = {}
            else:
                dmi = indicator_service.compute_dmi(high, low, close)

            if skip_sar:
                sar = []
            else:
                sar = indicator_service.compute_sar(high, low)

            if skip_obv:
                obv = []
            else:
                obv = indicator_service.compute_obv(close, vol)

            if skip_td:
                td = {}
            else:
                td = indicator_service.compute_td_setup_and_count(close)
        except Exception as e:
            logger.warning(f"è®¡ç®—{code}æŒ‡æ ‡å¤±è´¥: {e}")
            return []

        # ç»Ÿä¸€å½’ä¸€åŒ–é•¿åº¦åˆ° nï¼ˆå¦‚æœè·³è¿‡è®¡ç®—ï¼Œä»æ•°æ®åº“è¯»å–çš„æ•°æ®å·²ç»æ˜¯æ­£ç¡®æ ¼å¼ï¼Œä¸éœ€è¦å½’ä¸€åŒ–ï¼‰
        if not skip_expma:
            expma_map = self._norm_map(expma_map, n)
        if not skip_ma:
            ma_map = self._norm_map(ma_map, n)
        if not skip_macd:
            macd = {k: self._norm_list(v, n) for k, v in (macd or {}).items()}
        if not skip_rsi:
            rsi_map = self._norm_map(rsi_map, n)
        if not skip_kdj:
            kdj = {k: self._norm_list(v, n) for k, v in (kdj or {}).items()}
        if not skip_boll:
            boll = {k: self._norm_list(v, n) for k, v in (boll or {}).items()}
        if not skip_cci:
            cci = self._norm_list(cci or [], n)
        if not skip_wr:
            wr = self._norm_list(wr or [], n)
        if not skip_dmi:
            dmi = {k: self._norm_list(v, n) for k, v in (dmi or {}).items()}
        if not skip_sar:
            sar = self._norm_list(sar or [], n)
        if not skip_obv:
            obv = self._norm_list(obv or [], n)
        if not skip_td:
            td = {k: self._norm_list(v, n) for k, v in (td or {}).items()}

        # ğŸš€ ä»£ç é‡æ„ï¼šæå–æŒ‡æ ‡å­—æ®µæ˜ å°„é…ç½®ï¼Œæ¶ˆé™¤é‡å¤æ›´æ–°é€»è¾‘
        def get_indicator_field_configs():
            """è·å–æ‰€æœ‰æŒ‡æ ‡çš„å­—æ®µæ˜ å°„é…ç½®"""
            configs = []

            # EXPMAé…ç½®
            if not skip_expma:
                for p in self.expma_periods:
                    configs.append({
                        'source': expma_map.get(p, [None] * n),
                        'field': f'expma_{p}',
                        'type': 'float'
                    })

            # MAé…ç½®
            if not skip_ma:
                for p in self.ma_periods:
                    configs.append({
                        'source': ma_map.get(p, [None] * n),
                        'field': f'ma_{p}',
                        'type': 'float'
                    })

            # MACDé…ç½®
            if not skip_macd:
                macd_mappings = [("dif", "macd_dif"), ("dea", "macd_dea"), ("hist", "macd_histogram")]
                for k, col in macd_mappings:
                    configs.append({
                        'source': macd.get(k) or [None] * n,
                        'field': col,
                        'type': 'float'
                    })

            # RSIé…ç½®
            if not skip_rsi:
                rsi_mappings = [(6, "rsi_6"), (12, "rsi_12"), (24, "rsi_24")]
                for p, col in rsi_mappings:
                    configs.append({
                        'source': rsi_map.get(p) or [None] * n,
                        'field': col,
                        'type': 'float'
                    })

            # KDJé…ç½®
            if not skip_kdj:
                kdj_mappings = [("k", "kdj_k"), ("d", "kdj_d"), ("j", "kdj_j")]
                for k, col in kdj_mappings:
                    configs.append({
                        'source': kdj.get(k) or [None] * n,
                        'field': col,
                        'type': 'float'
                    })

            # BOLLé…ç½®
            if not skip_boll:
                boll_mappings = [("upper", "boll_upper"), ("middle", "boll_middle"), ("lower", "boll_lower")]
                for k, col in boll_mappings:
                    configs.append({
                        'source': boll.get(k) or [None] * n,
                        'field': col,
                        'type': 'float'
                    })

            # å•å€¼æŒ‡æ ‡é…ç½®
            single_indicators = [
                (not skip_cci, cci, "cci_14", 'float'),
                (not skip_wr, wr, "wr_14", 'float'),
                (not skip_sar, sar, "sar", 'float'),
                (not skip_obv, obv, "obv", 'float')
            ]

            for condition, source, field, type_name in single_indicators:
                if condition:
                    configs.append({
                        'source': source,
                        'field': field,
                        'type': type_name
                    })

            # DMIé…ç½®
            if not skip_dmi:
                dmi_mappings = [("pdi", "pdi_14"), ("mdi", "mdi_14"), ("adx", "adx_14"), ("adxr", "adxr_14")]
                for k, col in dmi_mappings:
                    configs.append({
                        'source': dmi.get(k) or [None] * n,
                        'field': col,
                        'type': 'float'
                    })

            # TDé…ç½®ï¼ˆç‰¹æ®Šå¤„ç†ï¼‰
            if not skip_td:
                configs.extend([
                    {
                        'source': td.get("setup") or [None] * n,
                        'field': 'td_setup',
                        'type': 'int'
                    },
                    {
                        'source': td.get("count") or [None] * n,
                        'field': 'td_count',
                        'type': 'int'
                    }
                ])

            return configs

        # è·å–æ‰€æœ‰æŒ‡æ ‡é…ç½®
        field_configs = get_indicator_field_configs()

        # ğŸ”§ ä¸ºæ¯ä¸ªå­—æ®µè®¡ç®—å¢é‡æ›´æ–°èµ·å§‹ä¸‹æ ‡ï¼šä»…åœ¨æœ€åä¸€æ¬¡å·²æœ‰å€¼ä¹‹åå¼€å§‹å†™å…¥
        field_start_indices: Dict[str, int] = {}
        if force_sync:
            for config in field_configs:
                field_start_indices[config['field']] = 0
        else:
            for config in field_configs:
                field_name = config['field']
                start_idx = 0
                for i in range(n - 1, -1, -1):
                    if kline_sorted[i].get(field_name) is not None:
                        start_idx = i + 1
                        break
                field_start_indices[field_name] = start_idx

        # è®¡ç®—å…¨å±€èµ·å§‹ä¸‹æ ‡ï¼šå°äºè¯¥ä½ç½®çš„è¡Œä¸ä¼šæœ‰ä»»ä½•å­—æ®µéœ€è¦æ›´æ–°ï¼Œç›´æ¥è·³è¿‡
        global_start_idx = 0
        if field_start_indices and not force_sync:
            global_start_idx = min(field_start_indices.values())

        updates: List[Dict[str, Any]] = []

        # æ‰¹é‡æ›´æ–°æŒ‡æ ‡å­—æ®µï¼ˆç»Ÿä¸€å¤„ç†é€»è¾‘ï¼Œä»…æ›´æ–°å°¾éƒ¨ç¼ºå¤±æˆ–å˜åŒ–çš„æ•°æ®ï¼‰
        for idx in range(global_start_idx, n):
            row = kline_sorted[idx]
            update_fields: Dict[str, Any] = {}

            # ğŸš€ ç»Ÿä¸€å­—æ®µæ›´æ–°é€»è¾‘ï¼šéå†æ‰€æœ‰é…ç½®ï¼Œç»Ÿä¸€å¤„ç†
            for config in field_configs:
                source = config['source']
                field = config['field']
                type_name = config['type']

                # å¢é‡æ›´æ–°ï¼šåœ¨è¯¥å­—æ®µæœ€åä¸€æ¬¡å·²æœ‰å€¼ä¹‹åæ‰å¼€å§‹å†™å…¥
                start_idx = field_start_indices.get(field, 0)
                if idx < start_idx:
                    continue

                val = self._sanitize_val(source[idx] if isinstance(source, list) else source)

                # ä¸å†æ¯”è¾ƒæ–°æ—§å€¼ï¼Œå¤„äºå¢é‡èŒƒå›´å†…çš„å­—æ®µç»Ÿä¸€å†™å…¥æ–°å€¼
                if val is not None:
                    if type_name == 'float':
                        update_fields[field] = float(val)
                    elif type_name == 'int':
                        update_fields[field] = int(val)
                    else:
                        update_fields[field] = val
                else:
                    # éœ€è¦æ˜¾å¼æ¸…ç©ºæ—§å€¼
                    update_fields[field] = None

            if update_fields:
                trade_date_str = row.get("trade_date")
                if trade_date_str:
                    updates.append({
                        "ts_code": code,
                        "period": period,
                        "trade_date": trade_date_str,
                        "year": int(trade_date_str[:4]),
                        "fields": update_fields
                    })

        return updates

    def _update_indicators_for_period(self, entity_type: str, entity_codes: List[str], period: str,
                                      force_sync: bool) -> int:
        if not entity_codes:
            return 0
        from app.constants.table_types import TableTypes
        table_type = TableTypes.entity_type_to_table_type(entity_type)
        if not table_type:
            return 0

        from threading import Lock
        from app.utils.concurrent_utils import process_concurrently
        import time

        # è¿›åº¦ç»Ÿè®¡
        start_time = time.time()
        total_codes = len(entity_codes)
        completed_codes = 0
        completed_codes_lock = Lock()
        updated_rows = 0
        updated_rows_lock = Lock()
        
        logger.info(
            f"å¼€å§‹æ›´æ–°{entity_type} {period}æŒ‡æ ‡ | "
            f"æ€»ä»£ç æ•°: {total_codes}"
        )

        def process_code_batch(code_batch: List[str]) -> int:
            nonlocal completed_codes  # å£°æ˜ä½¿ç”¨å¤–å±‚å˜é‡
            local_updated = 0
            # ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨æ ‡å‡†è¿æ¥æ± 
            db: Session = next(get_db())

            # ä¼˜åŒ–ï¼šç´¯ç§¯æ‰€æœ‰codeçš„æ›´æ–°ï¼Œç„¶ååˆ†æ‰¹æ‰§è¡Œï¼ˆæ§åˆ¶å•æ¬¡å†…å­˜å ç”¨ï¼‰
            all_batch_updates = []
            MAX_UPDATES_PER_FLUSH = 1000

            try:
                for code in code_batch:
                    try:
                        updates_for_code = self._build_indicator_updates_for_code(
                            code=code,
                            period=period,
                            table_type=table_type,
                            force_sync=force_sync,
                        )
                        if not updates_for_code:
                            continue

                        for update in updates_for_code:
                            all_batch_updates.append(update)
                            # æ§åˆ¶å•æ‰¹æ¬¡å†…å­˜å ç”¨ï¼šåˆ°è¾¾é˜ˆå€¼å³åˆ·å†™åˆ°æ•°æ®åº“
                            if len(all_batch_updates) >= MAX_UPDATES_PER_FLUSH:
                                local_updated += self._batch_upsert_indicators(db, table_type, all_batch_updates)
                                all_batch_updates.clear()

                    except Exception as e:
                        logger.error(f"å¤„ç†ä»£ç  {code} å¤±è´¥: {e}")
                    finally:
                        # æ›´æ–°è¿›åº¦è®¡æ•°ï¼ˆæ— è®ºæˆåŠŸæˆ–å¤±è´¥ï¼‰
                        with completed_codes_lock:
                            completed_codes += 1

                        # æ¯10ä¸ªä»£ç æˆ–æœ€åä¸€ä¸ªè¾“å‡ºè¿›åº¦
                        if completed_codes % 10 == 0 or completed_codes == total_codes:
                            elapsed = time.time() - start_time
                            progress_pct = (completed_codes / total_codes * 100)
                            avg_time = elapsed / completed_codes
                            remaining_codes = total_codes - completed_codes
                            remaining_time = avg_time * remaining_codes

                            logger.info(
                                f"è¿›åº¦: {completed_codes}/{total_codes} ({progress_pct:.1f}%) | "
                                f"å·²è€—æ—¶: {elapsed:.1f}ç§’ | "
                                f"é¢„è®¡å‰©ä½™: {remaining_time:.1f}ç§’"
                            )

                # ä¼˜åŒ–ï¼šæ‰§è¡Œå‰©ä½™æœªåˆ·æ–°çš„æ›´æ–°ï¼ˆå¯èƒ½ä¸è¶³ä¸€ä¸ªé˜ˆå€¼ï¼‰
                if all_batch_updates:
                    local_updated += self._batch_upsert_indicators(db, table_type, all_batch_updates)

                return local_updated

            finally:
                # ğŸ”§ ä¿®å¤è¿æ¥æ³„æ¼ï¼šç¡®ä¿è¿æ¥å§‹ç»ˆè¢«å…³é—­
                try:
                    db.close()
                except Exception as close_error:
                    logger.error(f"å…³é—­æ•°æ®åº“è¿æ¥å¤±è´¥: {close_error}")

        # åˆ†æ‰¹å¤„ç†ä»£ç ï¼Œé€‚å½“å‡å°æ‰¹æ¬¡è§„æ¨¡ä»¥é™ä½å•æ‰¹å†…å­˜å ç”¨
        batch_size = 50
        code_batches = [entity_codes[i:i + batch_size] for i in range(0, len(entity_codes), batch_size)]

        # ä½¿ç”¨å¹¶å‘å·¥å…·ç±»å¤„ç†ï¼Œé™åˆ¶å¹¶å‘çº¿ç¨‹æ•°ä»¥é™ä½å³°å€¼å†…å­˜
        max_workers = min(4, len(code_batches), (os.cpu_count() or 4))

        def process_with_lock(batch):
            nonlocal updated_rows
            result = process_code_batch(batch)
            with updated_rows_lock:
                updated_rows += result
            return result

        process_concurrently(
            code_batches,
            process_with_lock,
            max_workers=max_workers,
            error_handler=lambda batch, e: 0
        )
        
        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
        total_duration = time.time() - start_time
        logger.info(
            f"âœ… {entity_type} {period}æŒ‡æ ‡æ›´æ–°å®Œæˆ | "
            f"æ€»ä»£ç æ•°: {total_codes} | "
            f"æ›´æ–°è¡Œæ•°: {updated_rows} | "
            f"æ€»è€—æ—¶: {total_duration:.2f}ç§’ | "
            f"å¹³å‡æ¯ä»£ç : {total_duration/total_codes:.3f}ç§’"
        )

        return updated_rows

    @staticmethod
    def _batch_upsert_indicators(db: Session, table_type: str, batch_updates: List[Dict[str, Any]]) -> int:
        """æ‰¹é‡æ›´æ–°æŒ‡æ ‡å­—æ®µ - ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆåˆå¹¶å¤šå­—æ®µæ›´æ–°ï¼‰"""
        if not batch_updates:
            return 0

        # æŒ‰å¹´ä»½åˆ†ç»„
        year_groups = {}
        for update in batch_updates:
            year = update["year"]
            if year not in year_groups:
                year_groups[year] = []
            year_groups[year].append(update)

        total_updated = 0

        for year, updates in year_groups.items():
            # è·å–è¡¨æ¨¡å‹
            table_model = TableFactory.get_table_model(table_type, year)
            if not table_model:
                continue

            table_name = table_model.__tablename__

            # æŒ‰ (ts_code, period, trade_date) åˆ†ç»„ï¼Œåˆå¹¶æ‰€æœ‰å­—æ®µ
            record_groups = {}
            for update in updates:
                key = (update["ts_code"], update["period"], update["trade_date"])
                if key not in record_groups:
                    record_groups[key] = {}
                record_groups[key].update(update["fields"])

            # æ”¶é›†æ‰€æœ‰éœ€è¦æ›´æ–°çš„å­—æ®µåï¼ˆåŒ…æ‹¬ None å€¼ï¼Œç”¨äºæ¸…é™¤æ—§æ•°æ®ï¼‰
            # ä¼˜åŒ–ï¼šåŒæ—¶è®°å½•å“ªäº›å­—æ®µæœ‰é None å€¼ï¼Œç”¨äºåç»­ä¼˜åŒ– SQL è¯­å¥æ„å»º
            all_fields = set()
            field_has_value = {}  # è®°å½•æ¯ä¸ªå­—æ®µæ˜¯å¦æœ‰é None å€¼
            
            for fields in record_groups.values():
                for field_name, value in fields.items():
                    all_fields.add(field_name)
                    if value is not None:
                        field_has_value[field_name] = True

            if not all_fields:
                continue

            # æ³¨æ„ï¼šä¿ç•™æ‰€æœ‰å­—æ®µï¼ˆåŒ…æ‹¬å…¨ä¸º None çš„å­—æ®µï¼‰ï¼Œå› ä¸º None å€¼å¯èƒ½ç”¨äºæ¸…é™¤æ—§æ•°æ®
            # ä½†æˆ‘ä»¬å¯ä»¥æ ¹æ®å­—æ®µæ˜¯å¦æœ‰å€¼æ¥ä¼˜åŒ– SQL è¯­å¥æ„å»º
            effective_fields = list(all_fields)

            # åˆ†å—å¤„ç†ï¼Œæ¯æ‰¹300æ¡ï¼Œå‡å°å•æ¬¡ SQL ä½“ç§¯ï¼Œé™ä½å†…å­˜å‹åŠ›
            chunk_size = 300
            records_list = list(record_groups.items())
            
            # ä¼˜åŒ–ï¼šæ‰¹é‡äº‹åŠ¡æäº¤ï¼Œå‡å°‘ commit æ¬¡æ•°
            # å°†åŒä¸€å¹´çš„æ‰€æœ‰ chunk æ”¾åœ¨ä¸€ä¸ªäº‹åŠ¡ä¸­ï¼Œæœ€åç»Ÿä¸€ commit
            try:
                for i in range(0, len(records_list), chunk_size):
                    chunk = records_list[i:i + chunk_size]
                    
                    # ä¸ºæ¯ä¸ªå­—æ®µæ„å»º CASE WHEN è¯­å¥
                    # ä¼˜åŒ–ï¼šç»Ÿä¸€ä½¿ç”¨å‚æ•°ç»‘å®šå¤„ç†æ‰€æœ‰å€¼ï¼ˆåŒ…æ‹¬ Noneï¼‰ï¼ŒSQLAlchemy ä¼šè‡ªåŠ¨å°† None è½¬æ¢ä¸º NULL
                    field_updates = {}
                    where_parts = []
                    params = {}
                    
                    for j, ((ts_code, period, trade_date), fields) in enumerate(chunk):
                        # ä¼˜åŒ–ï¼šé¢„å…ˆæ„å»ºæ¡ä»¶å­—ç¬¦ä¸²ï¼Œé¿å…åœ¨æ¯ä¸ªå­—æ®µå¾ªç¯ä¸­é‡å¤æ„å»º
                        condition = f"ts_code = :ts_code_{j} AND period = :period_{j} AND trade_date = :trade_date_{j}"
                        
                        # WHERE æ¡ä»¶ï¼ˆå¸¦æ‹¬å·ç”¨äº OR ç»„åˆï¼‰
                        where_parts.append(f"({condition})")
                        params[f"ts_code_{j}"] = ts_code
                        params[f"period_{j}"] = period
                        params[f"trade_date_{j}"] = trade_date
                        
                        # ä¸ºæ¯ä¸ªæœ‰æ•ˆå­—æ®µæ·»åŠ  CASE WHENï¼ˆç»Ÿä¸€ä½¿ç”¨å‚æ•°ç»‘å®šï¼ŒåŒ…æ‹¬ None å€¼ï¼‰
                        for field_name in effective_fields:
                            value = fields.get(field_name)
                            
                            # ç»Ÿä¸€å¤„ç†ï¼šæ‰€æœ‰å€¼éƒ½ä½¿ç”¨å‚æ•°ç»‘å®šï¼ˆSQLAlchemy ä¼šå°† None è‡ªåŠ¨è½¬æ¢ä¸º NULLï¼‰
                            if field_name not in field_updates:
                                field_updates[field_name] = []
                            field_updates[field_name].append(
                                f"WHEN {condition} THEN :{field_name}_{j}"
                            )
                            params[f"{field_name}_{j}"] = value  # å¯ä»¥æ˜¯ Noneï¼ŒSQLAlchemy ä¼šæ­£ç¡®å¤„ç†

                    # æ„å»ºå®Œæ•´çš„ UPDATE è¯­å¥ï¼ˆä¸€æ¬¡æ›´æ–°æ‰€æœ‰å­—æ®µï¼‰
                    set_clauses = []
                    for field_name in effective_fields:
                        if field_updates.get(field_name):
                            case_when = " ".join(field_updates[field_name])
                            set_clauses.append(f"`{field_name}` = CASE {case_when} ELSE `{field_name}` END")

                    if not set_clauses:
                        continue

                    where_clause = " OR ".join(where_parts)
                    set_clause = ", ".join(set_clauses)

                    # æ‰§è¡Œæ›´æ–°
                    sql = text(f"""
                        UPDATE `{table_name}`
                        SET {set_clause}
                        WHERE {where_clause}
                    """)

                    result = db.execute(sql, params)
                    affected_rows = result.rowcount
                    total_updated += affected_rows

                # ä¼˜åŒ–ï¼šæ‰€æœ‰ chunk å®Œæˆåç»Ÿä¸€ commitï¼Œå‡å°‘äº‹åŠ¡æäº¤æ¬¡æ•°
                db.commit()
            except Exception as e:
                logger.error(f"æ‰¹é‡æ›´æ–°æŒ‡æ ‡å­—æ®µå¤±è´¥ï¼ˆå¹´ä»½ {year}ï¼‰: {e}")
                db.rollback()
                continue

        return total_updated


indicator_updater = TechnicalIndicatorUpdater()
